{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Atlassian Data Center Helm Charts \u00b6 This project contains Helm charts for installing Atlassian's Jira Data Center , Confluence Data Center , and Bitbucket Data Center on Kubernetes. Use the charts to install and operate Data Center products within a Kubernetes cluster of your choice. It can be a managed environment, such as Amazon EKS , Azure Kubernetes Service , Google Kubernetes Engine , or a custom on-premise system. Support disclaimer \u00b6 Warning We don\u2019t officially support the functionality described in the examples or the documented platforms . You should use them for reference only. Read more about what we support and what we don\u2019t . Note that Jira currently has limitations with scaling . Read more about this and other product and platform limitations . Architecture \u00b6 The diagram below provides a high level overview of what a typical deployment might look like when using the Atlassian Data Center Helm charts: Installing the Helm charts \u00b6 Prerequisites and setup - everything you need to do before installing the Helm charts Installation - the steps to install the Helm charts Migration - what you have to do if you're migrating an exisitng deployment to Kubernetes Additional content \u00b6 Operation - how to upgrade applications, scale your cluster, and update resources Configuration - a deep dive into the configuration parameters Platforms support - how to allow support for different platforms Examples - various configuration examples Troubleshooting - how to debug issues with installation Product versions \u00b6 The minimum versions that we support for each product are: Jira DC Confluence DC Bitbucket DC 8.19 7.13 7.12 Feedback \u00b6 If you find any issues, raise a ticket . If you have general feedback or questions regarding the charts, use Atlassian Community Kubernetes space . Contributions \u00b6 Contributions are welcome. Find out how to contribute . License \u00b6 Apache 2.0 licensed, see license file .","title":"Home"},{"location":"#atlassian-data-center-helm-charts","text":"This project contains Helm charts for installing Atlassian's Jira Data Center , Confluence Data Center , and Bitbucket Data Center on Kubernetes. Use the charts to install and operate Data Center products within a Kubernetes cluster of your choice. It can be a managed environment, such as Amazon EKS , Azure Kubernetes Service , Google Kubernetes Engine , or a custom on-premise system.","title":"Atlassian Data Center Helm Charts"},{"location":"#support-disclaimer","text":"Warning We don\u2019t officially support the functionality described in the examples or the documented platforms . You should use them for reference only. Read more about what we support and what we don\u2019t . Note that Jira currently has limitations with scaling . Read more about this and other product and platform limitations .","title":"Support disclaimer"},{"location":"#architecture","text":"The diagram below provides a high level overview of what a typical deployment might look like when using the Atlassian Data Center Helm charts:","title":"Architecture"},{"location":"#installing-the-helm-charts","text":"Prerequisites and setup - everything you need to do before installing the Helm charts Installation - the steps to install the Helm charts Migration - what you have to do if you're migrating an exisitng deployment to Kubernetes","title":"Installing the Helm charts"},{"location":"#additional-content","text":"Operation - how to upgrade applications, scale your cluster, and update resources Configuration - a deep dive into the configuration parameters Platforms support - how to allow support for different platforms Examples - various configuration examples Troubleshooting - how to debug issues with installation","title":"Additional content"},{"location":"#product-versions","text":"The minimum versions that we support for each product are: Jira DC Confluence DC Bitbucket DC 8.19 7.13 7.12","title":"Product versions"},{"location":"#feedback","text":"If you find any issues, raise a ticket . If you have general feedback or questions regarding the charts, use Atlassian Community Kubernetes space .","title":"Feedback"},{"location":"#contributions","text":"Contributions are welcome. Find out how to contribute .","title":"Contributions"},{"location":"#license","text":"Apache 2.0 licensed, see license file .","title":"License"},{"location":"examples/EXAMPLES/","text":"Available examples \u00b6 Support disclaimer Use the examples we provide as reference only, we don\u2019t offer official support for them. Kubernetes clusters \u00b6 See examples of provisioning Kubernetes clusters on cloud-based providers: Amazon EKS Google GKE Azure AKS Ingress \u00b6 See an example of provisioning an NGINX Ingress controller Database \u00b6 See an example of creating an Amazon RDS database instance Storage \u00b6 AWS EBS See an example of local storage utilizing AWS EBS-backed volumes AWS EFS See an example of shared storage utilizing AWS EFS-backed filesystem NFS See an example of standing up an NFS server for Bitbucket Elasticsearch \u00b6 See an example of standing up an Elasticsearch instance for Bitbucket Logging \u00b6 See an example of how to deploy an EFK stack to Kubernetes SSH \u00b6 See an example of SSH service in Bitbucket on Kubernetes Customization \u00b6 See an example of External libraries and plugins","title":"Available examples"},{"location":"examples/EXAMPLES/#available-examples","text":"Support disclaimer Use the examples we provide as reference only, we don\u2019t offer official support for them.","title":"Available examples"},{"location":"examples/EXAMPLES/#kubernetes-clusters","text":"See examples of provisioning Kubernetes clusters on cloud-based providers: Amazon EKS Google GKE Azure AKS","title":" Kubernetes clusters"},{"location":"examples/EXAMPLES/#ingress","text":"See an example of provisioning an NGINX Ingress controller","title":" Ingress"},{"location":"examples/EXAMPLES/#database","text":"See an example of creating an Amazon RDS database instance","title":" Database"},{"location":"examples/EXAMPLES/#storage","text":"AWS EBS See an example of local storage utilizing AWS EBS-backed volumes AWS EFS See an example of shared storage utilizing AWS EFS-backed filesystem NFS See an example of standing up an NFS server for Bitbucket","title":" Storage"},{"location":"examples/EXAMPLES/#elasticsearch","text":"See an example of standing up an Elasticsearch instance for Bitbucket","title":" Elasticsearch"},{"location":"examples/EXAMPLES/#logging","text":"See an example of how to deploy an EFK stack to Kubernetes","title":" Logging"},{"location":"examples/EXAMPLES/#ssh","text":"See an example of SSH service in Bitbucket on Kubernetes","title":" SSH"},{"location":"examples/EXAMPLES/#customization","text":"See an example of External libraries and plugins","title":" Customization"},{"location":"examples/cluster/AKS_SETUP/","text":"Preparing an AKS cluster \u00b6 This example provides instructions for creating a Kubernetes cluster using Azure AKS . Prerequisites \u00b6 We recommend installing and configuring the Azure Cloud Shell , allowing for CLI interaction with the AKS cluster. Manual creation \u00b6 Follow the Azure Kubernetes Service Quickstart for details on creating an AKS cluster. Next step - Ingress controller Having established a cluster, continue with provisioning the next piece of prerequisite infrastructure, the Ingress controller .","title":"Preparing an AKS cluster"},{"location":"examples/cluster/AKS_SETUP/#preparing-an-aks-cluster","text":"This example provides instructions for creating a Kubernetes cluster using Azure AKS .","title":"Preparing an AKS cluster"},{"location":"examples/cluster/AKS_SETUP/#prerequisites","text":"We recommend installing and configuring the Azure Cloud Shell , allowing for CLI interaction with the AKS cluster.","title":"Prerequisites"},{"location":"examples/cluster/AKS_SETUP/#manual-creation","text":"Follow the Azure Kubernetes Service Quickstart for details on creating an AKS cluster. Next step - Ingress controller Having established a cluster, continue with provisioning the next piece of prerequisite infrastructure, the Ingress controller .","title":"Manual creation"},{"location":"examples/cluster/CLOUD_PROVIDERS/","text":"Provisioning Kubernetes clusters on cloud-based providers \u00b6 Here are installation and configuration instructions for cloud providers: Amazon EKS Google GKE Azure AKS","title":"Provisioning Kubernetes clusters on cloud-based providers"},{"location":"examples/cluster/CLOUD_PROVIDERS/#provisioning-kubernetes-clusters-on-cloud-based-providers","text":"Here are installation and configuration instructions for cloud providers: Amazon EKS Google GKE Azure AKS","title":"Provisioning Kubernetes clusters on cloud-based providers"},{"location":"examples/cluster/EKS_SETUP/","text":"Preparing an EKS cluster \u00b6 This example provides instructions for creating a Kubernetes cluster using Amazon EKS . Prerequisites \u00b6 We recommend installing and configuring eksctl , allowing for CLI interaction with the EKS cluster. Manual creation \u00b6 Follow the Getting started with Amazon EKS for details on creating an EKS cluster. Or, using the ClusterConfig below as an example, deploy a K8s cluster with eksctl in ~20 minutes: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : atlassian-cluster region : ap-southeast-2 managedNodeGroups : - name : appNodes instanceType : m5.large desiredCapacity : 2 ssh : # enable SSH using SSM enableSsm : true Cluster considerations It's always a good idea to consider the following points before creating the cluster: Geographical region - where will the cluster reside. EC2 instance type - the instance type to be used for the nodes that make up the cluster. Number of nodes - guidance on the resource dimensions that should be used for these nodes can be found in Requests and limits . Adding the config above to a file named config.yaml provision the cluster: eksctl create cluster -f config.yaml Next step - Ingress controller Having established a cluster, continue with provisioning the next piece of prerequisite infrastructure, the Ingress controller .","title":"Preparing an EKS cluster"},{"location":"examples/cluster/EKS_SETUP/#preparing-an-eks-cluster","text":"This example provides instructions for creating a Kubernetes cluster using Amazon EKS .","title":"Preparing an EKS cluster"},{"location":"examples/cluster/EKS_SETUP/#prerequisites","text":"We recommend installing and configuring eksctl , allowing for CLI interaction with the EKS cluster.","title":"Prerequisites"},{"location":"examples/cluster/EKS_SETUP/#manual-creation","text":"Follow the Getting started with Amazon EKS for details on creating an EKS cluster. Or, using the ClusterConfig below as an example, deploy a K8s cluster with eksctl in ~20 minutes: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : atlassian-cluster region : ap-southeast-2 managedNodeGroups : - name : appNodes instanceType : m5.large desiredCapacity : 2 ssh : # enable SSH using SSM enableSsm : true Cluster considerations It's always a good idea to consider the following points before creating the cluster: Geographical region - where will the cluster reside. EC2 instance type - the instance type to be used for the nodes that make up the cluster. Number of nodes - guidance on the resource dimensions that should be used for these nodes can be found in Requests and limits . Adding the config above to a file named config.yaml provision the cluster: eksctl create cluster -f config.yaml Next step - Ingress controller Having established a cluster, continue with provisioning the next piece of prerequisite infrastructure, the Ingress controller .","title":"Manual creation"},{"location":"examples/cluster/GKE_SETUP/","text":"Preparing an GKE cluster \u00b6 This example provides instructions for creating a Kubernetes cluster using Google GKE . Prerequisites \u00b6 We recommend installing and configuring Google Cloud SDK , allowing for CLI interaction with an GKE cluster. Manual creation \u00b6 Follow the How-to guides for details on creating an GKE cluster. Next step - Ingress controller Having established a cluster, continue with provisioning the next piece of prerequisite infrastructure, the Ingress controller .","title":"Preparing an GKE cluster"},{"location":"examples/cluster/GKE_SETUP/#preparing-an-gke-cluster","text":"This example provides instructions for creating a Kubernetes cluster using Google GKE .","title":"Preparing an GKE cluster"},{"location":"examples/cluster/GKE_SETUP/#prerequisites","text":"We recommend installing and configuring Google Cloud SDK , allowing for CLI interaction with an GKE cluster.","title":"Prerequisites"},{"location":"examples/cluster/GKE_SETUP/#manual-creation","text":"Follow the How-to guides for details on creating an GKE cluster. Next step - Ingress controller Having established a cluster, continue with provisioning the next piece of prerequisite infrastructure, the Ingress controller .","title":"Manual creation"},{"location":"examples/database/AMAZON_RDS/","text":"Creating an RDS database instance \u00b6 This example provides instructions for creating an Amazon RDS DB instance . Prerequisites \u00b6 An AWS account , IAM user , VPC (or default VPC ) and security group are required before an RDS DB instance can be created. See Setting up for Amazon RDS for further instructions. Database creation \u00b6 There are two steps for creating the database: Initialize database server Initialize database and user 1. Initialize database server \u00b6 For details on standing up an RDS DB server follow the guide: Creating an Amazon RDS DB instance . 2. Initialize database and user \u00b6 Don't forget to create the database and user! This is a required step. For details on creating the application database and database user follow the appropriate guide below: Jira Create database for Jira Confluence Create database for Confluence Bitbucket Create database for Bitbucket Crowd Create database for Crowd Next step - Shared storage Having created the database continue with provisioning the next piece of prerequisite infrastructure, shared storage .","title":"Creating an RDS database instance"},{"location":"examples/database/AMAZON_RDS/#creating-an-rds-database-instance","text":"This example provides instructions for creating an Amazon RDS DB instance .","title":"Creating an RDS database instance"},{"location":"examples/database/AMAZON_RDS/#prerequisites","text":"An AWS account , IAM user , VPC (or default VPC ) and security group are required before an RDS DB instance can be created. See Setting up for Amazon RDS for further instructions.","title":"Prerequisites"},{"location":"examples/database/AMAZON_RDS/#database-creation","text":"There are two steps for creating the database: Initialize database server Initialize database and user","title":"Database creation"},{"location":"examples/database/AMAZON_RDS/#1-initialize-database-server","text":"For details on standing up an RDS DB server follow the guide: Creating an Amazon RDS DB instance .","title":"1. Initialize database server"},{"location":"examples/database/AMAZON_RDS/#2-initialize-database-and-user","text":"Don't forget to create the database and user! This is a required step. For details on creating the application database and database user follow the appropriate guide below: Jira Create database for Jira Confluence Create database for Confluence Bitbucket Create database for Bitbucket Crowd Create database for Crowd Next step - Shared storage Having created the database continue with provisioning the next piece of prerequisite infrastructure, shared storage .","title":"2. Initialize database and user"},{"location":"examples/database/CLOUD_PROVIDERS/","text":"Provisioning databases on cloud-based providers \u00b6 Supported databases Your selected database engine type must be supported by the Data Center product you wish to install: Jira Jira supported databases Confluence Confluence supported databases Bitbucket Bitbucket supported databases Crowd Crowd supported databases Database deployment and configuration instructions for cloud providers can be found below: Amazon RDS","title":"Provisioning databases on cloud-based providers"},{"location":"examples/database/CLOUD_PROVIDERS/#provisioning-databases-on-cloud-based-providers","text":"Supported databases Your selected database engine type must be supported by the Data Center product you wish to install: Jira Jira supported databases Confluence Confluence supported databases Bitbucket Bitbucket supported databases Crowd Crowd supported databases Database deployment and configuration instructions for cloud providers can be found below: Amazon RDS","title":"Provisioning databases on cloud-based providers"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/","text":"Bitbucket Elasticsearch recommendations \u00b6 While Bitbucket has its own internal Elasticsearch instance, we highly recommend you use an external Elasticsearch installation, either within the Kubernetes cluster or, if available, an instance managed by your hosting provider. Installing and configuring Elasticsearch in your Kubernetes cluster \u00b6 Installing Elasticsearch into your Kubernetes cluster \u00b6 Choose a version of Elasticsearch that is supported by the version of Bitbucket you are installing . For Bitbucket 7.14 the latest supported Elasticsearch version is 7.9.3, so we will target that. There are official Helm charts for Elasticsearch 7.9.3 . Following the documentation there add the Elasticsearch Helm charts repository: helm repo add elastic https://helm.elastic.co then install it: helm install elasticsearch --set imageTag = \"7.9.3\" elastic/elasticsearch Prerequisites of Elasticsearch Helm chart Running the above commands will install Elasticsearch with the default configuration, which is 3 worker nodes. However, it may not always work out of the box if failed to fulfill prerequisites for the default installation. Some example prerequisites include: CPU/memory requests: 1000m/2Gi (for each worker node) Preconfigured storage volumes (30Gi for each worker node) For more details refer to Elasticsearch values.yaml file . Configuring your Bitbucket deployment \u00b6 To enable the installed Elasticsearch service you need to configure the service URL under bitbucket: stanza in the values.yaml file. Check the Kubernetes official documentation on how to get DNS record for a service . bitbucket : elasticSearch : baseUrl : http://elasticsearch-master.<namespace>.svc.cluster.local:9200 This will also have the effect of disabling Bitbucket\u2019s internal Elasticsearch instance. Elasticsearch security If you have Elasticsearch cluster with security enabled , i.e. having credential details stored in a Kubernetes secret and passed into extraEnvs as this example does, you can then use the same secret and configure that in the bitbucket values.yaml file: bitbucket : elasticSearch : credentials : secretName : <my-elasticsearch-secret> usernameSecreyKey : username passwordSecretKey : password Read about Kubernetes secrets . Configuring Amazon Elasticsearch Service with Bitbucket on Kubernetes \u00b6 Creating an Amazon Elasticsearch Service domain with a master user \u00b6 The Elasticsearch instance (\u201cdomain\u201d) can be created via the AWS CLI or the web console; for this example we will use the web console and a master user: In the EKS console navigate to Your Cluster \u2192 Networking and note the VPC ID. In the Elasticsearch console create a new domain: Select a production deployment. Select Elasticsearch version 7.9. In the next screen configure the AZs and nodes as appropriate for your expected workload. On the Access and security page: Select the same VPC as the EKS cluster, as noted in step 1. Select appropriate subnets for each AZ; private subnets are fine. Select appropriate security groups that will grant node/pod access. Tick Fine\u2013grained access control : Select Create master user and add a username and a strong password. Configure tags, etc. as appropriate for your organisation. Once the Elasticsearch domain has finished creating, make a note of the VPC Endpoint , which will be an HTTPS URL. Configuring your Bitbucket deployment \u00b6 To use the managed Elasticsearch service, first create a Kubernetes secret using the username and password from step 4 above. Then configure the service URL under bitbucket: in the values.yaml file, substituting the values below from the above steps where appropriate: bitbucket : elasticSearch : baseUrl : <VPC Endpoint> credentials : secretName : <my-elasticsearch-secret> usernameSecreyKey : username passwordSecretKey : password Read about Kubernetes secrets . Testing your Elasticsearch connection \u00b6 To test if Elasticsearch is properly set up, go to Administration > System - Server settings . The Elasticsearch URL should be pre-populated already in the search section. Click the Test button to see if it connects successfully.","title":"Bitbucket Elasticsearch recommendations"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#bitbucket-elasticsearch-recommendations","text":"While Bitbucket has its own internal Elasticsearch instance, we highly recommend you use an external Elasticsearch installation, either within the Kubernetes cluster or, if available, an instance managed by your hosting provider.","title":"Bitbucket Elasticsearch recommendations"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#installing-and-configuring-elasticsearch-in-your-kubernetes-cluster","text":"","title":"Installing and configuring Elasticsearch in your Kubernetes cluster"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#installing-elasticsearch-into-your-kubernetes-cluster","text":"Choose a version of Elasticsearch that is supported by the version of Bitbucket you are installing . For Bitbucket 7.14 the latest supported Elasticsearch version is 7.9.3, so we will target that. There are official Helm charts for Elasticsearch 7.9.3 . Following the documentation there add the Elasticsearch Helm charts repository: helm repo add elastic https://helm.elastic.co then install it: helm install elasticsearch --set imageTag = \"7.9.3\" elastic/elasticsearch Prerequisites of Elasticsearch Helm chart Running the above commands will install Elasticsearch with the default configuration, which is 3 worker nodes. However, it may not always work out of the box if failed to fulfill prerequisites for the default installation. Some example prerequisites include: CPU/memory requests: 1000m/2Gi (for each worker node) Preconfigured storage volumes (30Gi for each worker node) For more details refer to Elasticsearch values.yaml file .","title":"Installing Elasticsearch into your Kubernetes cluster"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#configuring-your-bitbucket-deployment","text":"To enable the installed Elasticsearch service you need to configure the service URL under bitbucket: stanza in the values.yaml file. Check the Kubernetes official documentation on how to get DNS record for a service . bitbucket : elasticSearch : baseUrl : http://elasticsearch-master.<namespace>.svc.cluster.local:9200 This will also have the effect of disabling Bitbucket\u2019s internal Elasticsearch instance. Elasticsearch security If you have Elasticsearch cluster with security enabled , i.e. having credential details stored in a Kubernetes secret and passed into extraEnvs as this example does, you can then use the same secret and configure that in the bitbucket values.yaml file: bitbucket : elasticSearch : credentials : secretName : <my-elasticsearch-secret> usernameSecreyKey : username passwordSecretKey : password Read about Kubernetes secrets .","title":"Configuring your Bitbucket deployment"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#configuring-amazon-elasticsearch-service-with-bitbucket-on-kubernetes","text":"","title":"Configuring Amazon Elasticsearch Service with Bitbucket on Kubernetes"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#creating-an-amazon-elasticsearch-service-domain-with-a-master-user","text":"The Elasticsearch instance (\u201cdomain\u201d) can be created via the AWS CLI or the web console; for this example we will use the web console and a master user: In the EKS console navigate to Your Cluster \u2192 Networking and note the VPC ID. In the Elasticsearch console create a new domain: Select a production deployment. Select Elasticsearch version 7.9. In the next screen configure the AZs and nodes as appropriate for your expected workload. On the Access and security page: Select the same VPC as the EKS cluster, as noted in step 1. Select appropriate subnets for each AZ; private subnets are fine. Select appropriate security groups that will grant node/pod access. Tick Fine\u2013grained access control : Select Create master user and add a username and a strong password. Configure tags, etc. as appropriate for your organisation. Once the Elasticsearch domain has finished creating, make a note of the VPC Endpoint , which will be an HTTPS URL.","title":"Creating an Amazon Elasticsearch Service domain with a master user"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#configuring-your-bitbucket-deployment_1","text":"To use the managed Elasticsearch service, first create a Kubernetes secret using the username and password from step 4 above. Then configure the service URL under bitbucket: in the values.yaml file, substituting the values below from the above steps where appropriate: bitbucket : elasticSearch : baseUrl : <VPC Endpoint> credentials : secretName : <my-elasticsearch-secret> usernameSecreyKey : username passwordSecretKey : password Read about Kubernetes secrets .","title":"Configuring your Bitbucket deployment"},{"location":"examples/elasticsearch/BITBUCKET_ELASTICSEARCH/#testing-your-elasticsearch-connection","text":"To test if Elasticsearch is properly set up, go to Administration > System - Server settings . The Elasticsearch URL should be pre-populated already in the search section. Click the Test button to see if it connects successfully.","title":"Testing your Elasticsearch connection"},{"location":"examples/external_libraries/EXTERNAL_LIBS/","text":"External libraries and plugins \u00b6 .jar files only Whether loading external libraries, drivers or plugins, the approaches outlined here can only be used with .jar files. Plugin obr files can be extracted (unzipped) to access the included .jar In some situations, you may want to load 3rd party plugins, drivers or libraries so that they are available to the product being installed. An example of when this may be needed are for those products that do not ship with the appropriate MySQL and Oracle JDBC drivers. There are 3 strategies for doing this, you can either: use the required prerequisite shared home volume create a custom volume specifically for this purpose provide a custom command for the nfsPermissionFixer Each approach will be discussed below. Approach Which approach is used is totally up to you. For convenience you may want to just use shared-home , or if you'd like to keep things clean you may decide to mount these 3rd party libraries in a volume of their own. This approach would be particularly useful when these libraries need to be shared with other Pod's in your cluster. Shared home volume \u00b6 This approach consists of 3 high-level tasks: Create sub-dir in shared-home volume Copy libraries to sub-dir Update additionalLibraries stanza in values.yaml 1. Create sub-dir \u00b6 Add the Pod definition below to a file called shared-home-browser.yaml apiVersion : v1 kind : Pod metadata : name : shared-home-browser spec : containers : - name : browser image : debian:stable-slim volumeMounts : - mountPath : /shared-home name : shared-home command : [ \"bash\" , \"-c\" , \"--\" ] args : [ \"while true; do sleep 30; done;\" ] volumes : - name : shared-home persistentVolumeClaim : claimName : <shared-home-pvc-name> Initialise the Pod in the same namespace in which the shared-home PVC was created kubectl apply -f shared-home-browser.yaml Once running execute the following command, it will create the sub-sir, libraries , under /shared-home kubectl exec -it shared-home-browser -- bash -c \"mkdir -p /shared-home/libraries\" 2. Copy libraries to sub-dir \u00b6 Now copy the files you require to the sub-dir by using the kubectl cp command kubectl cp my_library.jar shared-home-browser:/shared-home/libraries 3. Update values.yaml \u00b6 Update the stanza, additionalLibraries , in values.yaml accordingly: additionalLibraries : - volumeName : shared-home subDirectory : libraries fileName : my_library.jar With this config these files ( my_library.jar ) will be injected into the container directory <product-installation-directory>/lib . For more info on how these files are injected into the appropriate product container location, see Jira's helper jira.additionalLibraries . Custom volume \u00b6 This approach is very similar to the Shared home volume approach, only a custom volume is created and used as opposed shared-home . Create a new volume for storing 3rd party libraries Create sub-dir for the new volume Copy libraries to sub-dir Update additionalLibraries stanza in values.yaml Update additionalVolumeMounts stanza in values.yaml Update additional stanza in values.yaml Steps Because many of the steps for this approach are similar to the steps used for Shared home volume only those that differ will be discussed. 1. Create new volume \u00b6 Using the same approach taken for provisioning the shared-home volume , create a new EFS with a corresponding PV and PVC . ReadOnlyMany Ensure that the PV and PVC are setup with ReadOnlyMany access 2. Update values.yaml \u00b6 Assuming that the PVC representing the EFS is called third-party-libraries , update the values.yaml so that the PVC is added as an additional mount: volumes : additional : - name : third-party-libraries persistentVolumeClaim : claimName : third-party-libraries Now add this as an additionalVolumeMounts additionalVolumeMounts : - volumeName : third-party-libraries mountPath : /libraries Finally inject the desired libraries by defining them under additionalLibraries additionalLibraries : - volumeName : third-party-libraries subDirectory : database_drivers fileName : my_library.jar Custom command \u00b6 This example is based on the GitHub issue discussed here . The nfsPermissionFixer in the values.yaml is used for appropriately setting the permissions on the shared-home volume. The command it uses for this is already defined by default, however it can also be supplied with a custom command for adding 3rd party libraries to shared-home . The example below shows how this approach can be used for adding the JDBC MySQL driver: 1 2 3 4 5 6 7 8 9 10 nfsPermissionFixer : command : | if [[ ! -f /shared-home/drivers/mysql-driver.jar ]]; then mkdir -p /shared-home/drivers apk add dpkg wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java_8.0.26-1debian10_all.deb dpkg-deb -R mysql-connector-java_8.0.26-1debian10_all.deb /tmp/ cp /tmp/usr/share/java/mysql-connector-java-8.0.26.jar /shared-home/drivers/mysql-driver.jar fi chgrp 2003 /shared-home; chmod g+w /shared-home shared-home permissions If taking this approach ensure the last thing your custom command does is apply the relevant permissions to the shared-home mount, see line 10 in yaml snippet above. Each product chart has a sharedHome.permissionFix.command helper for doing this. look at Jira's helper sharedHome.permissionFix.command for more details on how these permissions are applied by default. Remember to also update the additionalLibraries stanza accordingly: additionalLibraries : - volumeName : shared-home subDirectory : drivers fileName : mysql-driver.jar","title":"External libraries and plugins"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#external-libraries-and-plugins","text":".jar files only Whether loading external libraries, drivers or plugins, the approaches outlined here can only be used with .jar files. Plugin obr files can be extracted (unzipped) to access the included .jar In some situations, you may want to load 3rd party plugins, drivers or libraries so that they are available to the product being installed. An example of when this may be needed are for those products that do not ship with the appropriate MySQL and Oracle JDBC drivers. There are 3 strategies for doing this, you can either: use the required prerequisite shared home volume create a custom volume specifically for this purpose provide a custom command for the nfsPermissionFixer Each approach will be discussed below. Approach Which approach is used is totally up to you. For convenience you may want to just use shared-home , or if you'd like to keep things clean you may decide to mount these 3rd party libraries in a volume of their own. This approach would be particularly useful when these libraries need to be shared with other Pod's in your cluster.","title":"External libraries and plugins"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#shared-home-volume","text":"This approach consists of 3 high-level tasks: Create sub-dir in shared-home volume Copy libraries to sub-dir Update additionalLibraries stanza in values.yaml","title":"Shared home volume"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#1-create-sub-dir","text":"Add the Pod definition below to a file called shared-home-browser.yaml apiVersion : v1 kind : Pod metadata : name : shared-home-browser spec : containers : - name : browser image : debian:stable-slim volumeMounts : - mountPath : /shared-home name : shared-home command : [ \"bash\" , \"-c\" , \"--\" ] args : [ \"while true; do sleep 30; done;\" ] volumes : - name : shared-home persistentVolumeClaim : claimName : <shared-home-pvc-name> Initialise the Pod in the same namespace in which the shared-home PVC was created kubectl apply -f shared-home-browser.yaml Once running execute the following command, it will create the sub-sir, libraries , under /shared-home kubectl exec -it shared-home-browser -- bash -c \"mkdir -p /shared-home/libraries\"","title":"1. Create sub-dir"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#2-copy-libraries-to-sub-dir","text":"Now copy the files you require to the sub-dir by using the kubectl cp command kubectl cp my_library.jar shared-home-browser:/shared-home/libraries","title":"2. Copy libraries to sub-dir"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#3-update-valuesyaml","text":"Update the stanza, additionalLibraries , in values.yaml accordingly: additionalLibraries : - volumeName : shared-home subDirectory : libraries fileName : my_library.jar With this config these files ( my_library.jar ) will be injected into the container directory <product-installation-directory>/lib . For more info on how these files are injected into the appropriate product container location, see Jira's helper jira.additionalLibraries .","title":"3. Update values.yaml"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#custom-volume","text":"This approach is very similar to the Shared home volume approach, only a custom volume is created and used as opposed shared-home . Create a new volume for storing 3rd party libraries Create sub-dir for the new volume Copy libraries to sub-dir Update additionalLibraries stanza in values.yaml Update additionalVolumeMounts stanza in values.yaml Update additional stanza in values.yaml Steps Because many of the steps for this approach are similar to the steps used for Shared home volume only those that differ will be discussed.","title":"Custom volume"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#1-create-new-volume","text":"Using the same approach taken for provisioning the shared-home volume , create a new EFS with a corresponding PV and PVC . ReadOnlyMany Ensure that the PV and PVC are setup with ReadOnlyMany access","title":"1. Create new volume"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#2-update-valuesyaml","text":"Assuming that the PVC representing the EFS is called third-party-libraries , update the values.yaml so that the PVC is added as an additional mount: volumes : additional : - name : third-party-libraries persistentVolumeClaim : claimName : third-party-libraries Now add this as an additionalVolumeMounts additionalVolumeMounts : - volumeName : third-party-libraries mountPath : /libraries Finally inject the desired libraries by defining them under additionalLibraries additionalLibraries : - volumeName : third-party-libraries subDirectory : database_drivers fileName : my_library.jar","title":"2. Update values.yaml"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#custom-command","text":"This example is based on the GitHub issue discussed here . The nfsPermissionFixer in the values.yaml is used for appropriately setting the permissions on the shared-home volume. The command it uses for this is already defined by default, however it can also be supplied with a custom command for adding 3rd party libraries to shared-home . The example below shows how this approach can be used for adding the JDBC MySQL driver: 1 2 3 4 5 6 7 8 9 10 nfsPermissionFixer : command : | if [[ ! -f /shared-home/drivers/mysql-driver.jar ]]; then mkdir -p /shared-home/drivers apk add dpkg wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java_8.0.26-1debian10_all.deb dpkg-deb -R mysql-connector-java_8.0.26-1debian10_all.deb /tmp/ cp /tmp/usr/share/java/mysql-connector-java-8.0.26.jar /shared-home/drivers/mysql-driver.jar fi chgrp 2003 /shared-home; chmod g+w /shared-home shared-home permissions If taking this approach ensure the last thing your custom command does is apply the relevant permissions to the shared-home mount, see line 10 in yaml snippet above. Each product chart has a sharedHome.permissionFix.command helper for doing this. look at Jira's helper sharedHome.permissionFix.command for more details on how these permissions are applied by default. Remember to also update the additionalLibraries stanza accordingly: additionalLibraries : - volumeName : shared-home subDirectory : drivers fileName : mysql-driver.jar","title":"Custom command"},{"location":"examples/ingress/CONTROLLERS/","text":"Provisioning an Ingress controller \u00b6 In order for the provided Ingress resource to work, your Kubernetes cluster must have an ingress controller running. The Atlassian Helm charts have been tested with the NGINX Ingress Controller , however alternatives can also be used . Here is an example of how these controllers can be installed and configured for use with the Atlassian Helm charts: NGINX Ingress Controller","title":"Provisioning an Ingress controller"},{"location":"examples/ingress/CONTROLLERS/#provisioning-an-ingress-controller","text":"In order for the provided Ingress resource to work, your Kubernetes cluster must have an ingress controller running. The Atlassian Helm charts have been tested with the NGINX Ingress Controller , however alternatives can also be used . Here is an example of how these controllers can be installed and configured for use with the Atlassian Helm charts: NGINX Ingress Controller","title":"Provisioning an Ingress controller"},{"location":"examples/ingress/DNS/","text":"Create DNS record via AWS CLI \u00b6 DNS record creation using Route53 The approach below shows how a DNS record can be created using AWS Route53 and the AWS CLI for record sets First, identify the name of the auto provisioned AWS Classic Load Balancer that was created above for Step 2. Install controller : kubectl get service -n ingress | grep ingress-nginx | awk '{print $4}' | head -1 the output of this command should be the name of the load balancer, take note of the name i.e. b834z142d8118406795a34df35e10b17-38927090.eu-west-1.elb.amazonaws.com Next, using the first part of the load balancer name, get the HostedZoneId for the load balancer aws elb describe-load-balancers --load-balancer-name b834z142d8118406795a34df35e10b17 --region <aws_region> | jq '.LoadBalancerDescriptions[] | .CanonicalHostedZoneNameID' With the HostedZoneId and the full name of the load balancer create the JSON \"change batch\" file below: { \"Comment\" : \"An alias resource record for Jira in K8s\" , \"Changes\" : [ { \"Action\" : \"CREATE\" , \"ResourceRecordSet\" : { \"Name\" : <DNS record name> , \"Type\" : \"A\" , \"AliasTarget\" : { \"HostedZoneId\" : <Load balancer hosted zone ID> , \"DNSName\" : <Load balancer name> , \"EvaluateTargetHealth\" : true } } } ] } DNS record name If for example, the DNS record name were set to product.k8s.hoolicorp.com then the host, hoolicorp.com , would be the pre-registerd AWS Route53 hosted zone . Next get the zone ID for the hosted zone: aws route53 list-hosted-zones-by-name | jq '.HostedZones[] | select(.Name == \"hoolicorp.com.\") | .Id' Finally, using the hosted zone ID and the JSON change batch file created above, initialize the record: aws route53 change-resource-record-sets --hosted-zone-id <hosted zone ID> --change-batch file://change-batch.json This will return a response similar to the one below: { \"ChangeInfo\" : { \"Id\" : \"/change/C03268442VMV922ROD1M4\" , \"Status\" : \"PENDING\" , \"SubmittedAt\" : \"2021-08-30T01:42:23.478Z\" , \"Comment\" : \"An alias resource record for Jira in K8s\" } } You can get the current status of the record's initialization: aws route53 get-change --id /change/C03268442VMV922ROD1M4 Once the Status has transitioned to INSYNC the record is ready for use... { \"ChangeInfo\" : { \"Id\" : \"/change/C03268442VMV922ROD1M4\" , \"Status\" : \"INSYNC\" , \"SubmittedAt\" : \"2021-08-30T01:42:23.478Z\" , \"Comment\" : \"Creating Alias resource record sets in Route 53\" } }","title":"Create DNS record via AWS CLI"},{"location":"examples/ingress/DNS/#create-dns-record-via-aws-cli","text":"DNS record creation using Route53 The approach below shows how a DNS record can be created using AWS Route53 and the AWS CLI for record sets First, identify the name of the auto provisioned AWS Classic Load Balancer that was created above for Step 2. Install controller : kubectl get service -n ingress | grep ingress-nginx | awk '{print $4}' | head -1 the output of this command should be the name of the load balancer, take note of the name i.e. b834z142d8118406795a34df35e10b17-38927090.eu-west-1.elb.amazonaws.com Next, using the first part of the load balancer name, get the HostedZoneId for the load balancer aws elb describe-load-balancers --load-balancer-name b834z142d8118406795a34df35e10b17 --region <aws_region> | jq '.LoadBalancerDescriptions[] | .CanonicalHostedZoneNameID' With the HostedZoneId and the full name of the load balancer create the JSON \"change batch\" file below: { \"Comment\" : \"An alias resource record for Jira in K8s\" , \"Changes\" : [ { \"Action\" : \"CREATE\" , \"ResourceRecordSet\" : { \"Name\" : <DNS record name> , \"Type\" : \"A\" , \"AliasTarget\" : { \"HostedZoneId\" : <Load balancer hosted zone ID> , \"DNSName\" : <Load balancer name> , \"EvaluateTargetHealth\" : true } } } ] } DNS record name If for example, the DNS record name were set to product.k8s.hoolicorp.com then the host, hoolicorp.com , would be the pre-registerd AWS Route53 hosted zone . Next get the zone ID for the hosted zone: aws route53 list-hosted-zones-by-name | jq '.HostedZones[] | select(.Name == \"hoolicorp.com.\") | .Id' Finally, using the hosted zone ID and the JSON change batch file created above, initialize the record: aws route53 change-resource-record-sets --hosted-zone-id <hosted zone ID> --change-batch file://change-batch.json This will return a response similar to the one below: { \"ChangeInfo\" : { \"Id\" : \"/change/C03268442VMV922ROD1M4\" , \"Status\" : \"PENDING\" , \"SubmittedAt\" : \"2021-08-30T01:42:23.478Z\" , \"Comment\" : \"An alias resource record for Jira in K8s\" } } You can get the current status of the record's initialization: aws route53 get-change --id /change/C03268442VMV922ROD1M4 Once the Status has transitioned to INSYNC the record is ready for use... { \"ChangeInfo\" : { \"Id\" : \"/change/C03268442VMV922ROD1M4\" , \"Status\" : \"INSYNC\" , \"SubmittedAt\" : \"2021-08-30T01:42:23.478Z\" , \"Comment\" : \"Creating Alias resource record sets in Route 53\" } }","title":"Create DNS record via AWS CLI"},{"location":"examples/ingress/INGRESS_NGINX/","text":"NGINX Ingress Controller - with TLS termination \u00b6 NGINX ingress controller with automatic TLS certificate management using cert-manager and certificates from Let's Encrypt . Using these instructions These instructions are for reference purposes, as such they should be used for development and testing purposes only! See the official instructions for Deploying and configuring the controller . These instructions are composed of 3 high-level parts: Controller installation and configuration Certificate manager installation and configuration Ingress resource configuration Controller installation and configuration \u00b6 We recommend installing the controller using its official Helm Charts . You can also use the instructions below. 1. Add controller repository \u00b6 Add the ingress-nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Update the repository: helm repo update 2. Install controller \u00b6 Create a new namespace for the Ingress controller: kubectl create namespace ingress Install the controller using Helm: helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress This will take couple of minutes. Confirm your ingress controller is installed: kubectl get pods --namespace ingress 3. DNS setup \u00b6 Manually provision a new DNS record via your cloud provider, for instance AWS and Route53 , or dynamically using external-dns . There are also instructions on how this can be done using the AWS CLI . Once created, associate the DNS record with the auto provisioned load balancer that was created in Step 2. above . To do this first identify the name of the auto provisioned LB, this can be done by examining the deployed ingress services i.e.: kubectl get service -n ingress | grep ingress-nginx the output of this command should look something like... ingress-nginx-controller LoadBalancer 10 .100.22.16 b834z142d8118406795a34df35e10b17-38927090.eu-west-1.elb.amazonaws.com 80 :32615/TCP,443:31787/TCP 76m ingress-nginx-controller-admission ClusterIP 10 .100.5.36 <none> 443 /TCP 76m Take note of the LoadBalancer and using it as a value update the DNS record so that traffic is routed to it. It can take a few minutes for the DNS to resolve these changes. Certificate manager installation and configuration \u00b6 Kubernetes certificate management is handled using cert-manager . 1. Install cert-manager \u00b6 Add the cert-manager repository helm repo add jetstack https://charts.jetstack.io Update repositories helm repo update Install the cert-manager using Helm helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.3.1 \\ --set installCRDs = true Confirm the cert-manager is appropriately installed: kubectl get pods --namespace cert-manager 2. Create certificate issuer \u00b6 Using the yaml specification below create and apply the certificate Issuer resource: Namespace co-location Ensure that the certificate issuer is installed in the same namespace that the Atlassian product will be deployed to. apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-prod namespace : <product_deployment_namespace> spec : acme : # The ACME server URL server : https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : <user_email> # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-prod # Enable the HTTP-01 challenge provider solvers : - http01 : ingress : class : nginx Install the Issuer resource: kubectl apply -f issuer.yaml Ingress resource configuration \u00b6 Now that the Ingress controller and certificate manager are setup the Ingress resource can be configured accordingly by updating the values.yaml . 1. Ingress resource config \u00b6 For TLS cert auto-provisioning and TLS termination update the ingress stanza within the products values.yaml : ingress : create : true nginx : true maxBodySize : 250m host : <dns_record> path : \"/\" annotations : cert-manager.io/issuer : \"letsencrypt-prod\" # Using https://letsencrypt.org/ https : true tlsSecretName : tls-certificate Configuring the host value In this case the <dns_record> would correspond to the record name that was created in 3. DNS setup above Bitbucket SSH configuration \u00b6 Additional configuration Bitbucket requires additional Ingress config to allow for SSH access. See NGINX Ingress controller config for SSH connections for details. Next step - Database Having created the Ingress controller continue with provisioning the next piece of prerequisite infrastructure, the database .","title":"NGINX Ingress Controller - with TLS termination"},{"location":"examples/ingress/INGRESS_NGINX/#nginx-ingress-controller-with-tls-termination","text":"NGINX ingress controller with automatic TLS certificate management using cert-manager and certificates from Let's Encrypt . Using these instructions These instructions are for reference purposes, as such they should be used for development and testing purposes only! See the official instructions for Deploying and configuring the controller . These instructions are composed of 3 high-level parts: Controller installation and configuration Certificate manager installation and configuration Ingress resource configuration","title":"NGINX Ingress Controller - with TLS termination"},{"location":"examples/ingress/INGRESS_NGINX/#controller-installation-and-configuration","text":"We recommend installing the controller using its official Helm Charts . You can also use the instructions below.","title":"Controller installation and configuration"},{"location":"examples/ingress/INGRESS_NGINX/#1-add-controller-repository","text":"Add the ingress-nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Update the repository: helm repo update","title":"1. Add controller repository"},{"location":"examples/ingress/INGRESS_NGINX/#2-install-controller","text":"Create a new namespace for the Ingress controller: kubectl create namespace ingress Install the controller using Helm: helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress This will take couple of minutes. Confirm your ingress controller is installed: kubectl get pods --namespace ingress","title":"2. Install controller"},{"location":"examples/ingress/INGRESS_NGINX/#3-dns-setup","text":"Manually provision a new DNS record via your cloud provider, for instance AWS and Route53 , or dynamically using external-dns . There are also instructions on how this can be done using the AWS CLI . Once created, associate the DNS record with the auto provisioned load balancer that was created in Step 2. above . To do this first identify the name of the auto provisioned LB, this can be done by examining the deployed ingress services i.e.: kubectl get service -n ingress | grep ingress-nginx the output of this command should look something like... ingress-nginx-controller LoadBalancer 10 .100.22.16 b834z142d8118406795a34df35e10b17-38927090.eu-west-1.elb.amazonaws.com 80 :32615/TCP,443:31787/TCP 76m ingress-nginx-controller-admission ClusterIP 10 .100.5.36 <none> 443 /TCP 76m Take note of the LoadBalancer and using it as a value update the DNS record so that traffic is routed to it. It can take a few minutes for the DNS to resolve these changes.","title":"3. DNS setup"},{"location":"examples/ingress/INGRESS_NGINX/#certificate-manager-installation-and-configuration","text":"Kubernetes certificate management is handled using cert-manager .","title":"Certificate manager installation and configuration"},{"location":"examples/ingress/INGRESS_NGINX/#1-install-cert-manager","text":"Add the cert-manager repository helm repo add jetstack https://charts.jetstack.io Update repositories helm repo update Install the cert-manager using Helm helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.3.1 \\ --set installCRDs = true Confirm the cert-manager is appropriately installed: kubectl get pods --namespace cert-manager","title":"1. Install cert-manager"},{"location":"examples/ingress/INGRESS_NGINX/#2-create-certificate-issuer","text":"Using the yaml specification below create and apply the certificate Issuer resource: Namespace co-location Ensure that the certificate issuer is installed in the same namespace that the Atlassian product will be deployed to. apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-prod namespace : <product_deployment_namespace> spec : acme : # The ACME server URL server : https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email : <user_email> # Name of a secret used to store the ACME account private key privateKeySecretRef : name : letsencrypt-prod # Enable the HTTP-01 challenge provider solvers : - http01 : ingress : class : nginx Install the Issuer resource: kubectl apply -f issuer.yaml","title":"2. Create certificate issuer"},{"location":"examples/ingress/INGRESS_NGINX/#ingress-resource-configuration","text":"Now that the Ingress controller and certificate manager are setup the Ingress resource can be configured accordingly by updating the values.yaml .","title":"Ingress resource configuration"},{"location":"examples/ingress/INGRESS_NGINX/#1-ingress-resource-config","text":"For TLS cert auto-provisioning and TLS termination update the ingress stanza within the products values.yaml : ingress : create : true nginx : true maxBodySize : 250m host : <dns_record> path : \"/\" annotations : cert-manager.io/issuer : \"letsencrypt-prod\" # Using https://letsencrypt.org/ https : true tlsSecretName : tls-certificate Configuring the host value In this case the <dns_record> would correspond to the record name that was created in 3. DNS setup above","title":"1. Ingress resource config"},{"location":"examples/ingress/INGRESS_NGINX/#bitbucket-ssh-configuration","text":"Additional configuration Bitbucket requires additional Ingress config to allow for SSH access. See NGINX Ingress controller config for SSH connections for details. Next step - Database Having created the Ingress controller continue with provisioning the next piece of prerequisite infrastructure, the database .","title":"Bitbucket SSH configuration"},{"location":"examples/logging/efk/EFK/","text":"Logging in a Kubernetes environment \u00b6 Warning This functionality is not officially supported. This document explains how to enable aggregated logging in your Kubernetes cluster. There are many ways to do this and this document showcases only a few of the options. EFK stack \u00b6 A common Kubernetes logging pattern is the combination of Elasticsearch , Fluentd , and Kibana , known as EFK Stack . Fluentd is an open-source and multi-platform log processor that collects data/logs from different sources, aggregates, and forwards them to multiple destinations. It is fully compatible with Docker and Kubernetes environments. Elasticsearch is a distributed open search and analytics engine for all types of data. Kibana is an open-source front-end application that sits on top of Elasticsearch, providing search and data visualization capabilities for data indexed in Elasticsearch. There are different methods to deploy an EFK stack. We provide two deployment methods, the first is deploying EFK locally on Kubernetes, and the second is using a managed Elasticsearch instance outside the Kubernetes cluster. Local EFK stack \u00b6 This solution deploys the EFK stack inside the Kubernetes cluster. By setting fluentd.enabled value to true , Helm installs Fluentd on each of application pods. This means that after deployment all the product pods run Fluentd, which collects all the log files and sends them to the Fluentd aggregator container. To complete the EFK stack you need to install an Elasticsearch cluster and Kibana, and successfully forward the aggregated datalog to Elasticsearch using Fluentd, which is already installed. Follow these steps to install Elasticsearch 1. Install Elasticsearch \u00b6 Install Elasticsearch using the instructions documented here . Once installed make sure Elasticsearch cluster is working as expected by first port forwarding the service kubectl port-forward svc/elasticsearch-master 9200 you can then curl the endpoint for the current state $ curl localhost:9200 { \"name\" : \"elasticsearch-master-0\" , \"cluster_name\" : \"elasticsearch\" , \"cluster_uuid\" : \"uNdYC-2nSdWVdzPCw9P7jQ\" , \"version\" : { \"number\" : \"7.12.0\" , \"build_flavor\" : \"default\" , \"build_type\" : \"docker\" , \"build_hash\" : \"78722783c38caa25a70982b5b042074cde5d3b3a\" , \"build_date\" : \"2021-03-18T06:17:15.410153305Z\" , \"build_snapshot\" : false, \"lucene_version\" : \"8.8.0\" , \"minimum_wire_compatibility_version\" : \"6.8.0\" , \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" } , \"tagline\" : \"You Know, for Search\" } 2. Enable Fluentd \u00b6 Now enable Fluentd and set the hostname for Elasticsearch in values.yaml as follows: fluentd : enabled : true elasticsearch : hostname : elasticsearch-master Fluentd tries to parse and send the data to Elasticsearch, but since it's not installed the data is lost. At this point you have logged data in the installed Elasticsearch, and you should install Kibana to complete the EFK stack deployment: 3. Install Kibana \u00b6 helm install kibana elastic/kibana Make sure kibana is running by checking the deployment kubectl get deployment You should see something like... NAME READY UP-TO-DATE AVAILABLE AGE helm-operator 1 /1 1 1 23m ingress-nginx-release-controller 1 /1 1 1 22m kibana-kibana 1 /1 1 1 25m Through port-forwarding you can access Kibana via http://localhost:5601 kubectl port-forward deployment/kibana-kibana 5601 To visualise the logs you need to create an index pattern and then look at the the data in the discovery part. To create the index pattern go to Management \u2192 Stack Management and then select Kibana \u2192 Index Patterns . Managed EFK stack \u00b6 In this solution Elasticsearch is deployed as a managed AWS service and lives outside of the Kubernetes cluster. This approach uses Fluentbit instead of Fluentd for log processing. Fluentbit Fluentbit is used to collect and aggregate log data inside the EKS cluster. It then sends this to an AWS Elasticsearch instance outside of the cluster. When a node inside an EKS cluster needs to call an AWS API, it needs to provide extended permissions. Amazon provides an image of Fluentbit that supports AWS service accounts,and using this you no longer need to follow the traditional way. All you need is to have an IAM role for the AWS service account on an EKS cluster. Using this service account, an AWS permission can be provided to the containers in any pod that use that service account. The result is that the pods on that node can call AWS APIs. Your first step is to configure IAM roles for Service Accounts (IRSA) for Fluentbit , to make sure you have an OIDC identity provider to use IAM roles for the service account in the cluster: eksctl utils associate-iam-oidc-provider \\ --cluster <cluster_name> \\ --approve Then create an IAM policy to limit the permissions to connect to the Elasticsearch cluster. Before this, you need to set the following environment variables: Environment variable Value KUBE_NAMESPACE The namespace for kubernetes cluster ES_DOMAIN_NAME Elasticsearch domain name ES_VERSION Elasticsearch version ES_USER Elasticsearch username ES_PASSWORD Elasticsearch password (eg. export ES_PASSWORD=\"$(openssl rand -base64 8)_Ek1$\" ) ACCOUNT_ID AWS Account ID AWS_REGION AWS region code Now create the file fluent-bit-policy.json to define the policy itself: cat <<EoF > ~/environment/logging/fluent-bit-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"es:ESHttp*\" ], \"Resource\": \"arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}\", \"Effect\": \"Allow\" } ] } EoF Next initialize the policy: aws iam create-policy \\ --policy-name fluent-bit-policy \\ --policy-document file://~/environment/logging/fluent-bit-policy.json Create an IAM role for the service account: eksctl create iamserviceaccount \\ --name fluent-bit \\ --namespace dcd \\ --cluster dcd-ap-southeast-2 \\ --attach-policy-arn \"arn:aws:iam:: ${ ACCOUNT_ID } :policy/fluent-bit-policy\" \\ --approve \\ --override-existing-serviceaccounts Confirm that the service account with an Amazon Resource Name (ARN) of the IAM role is annotated: kubectl describe serviceaccount fluent-bit Look for output similar to: Name : fluent-bit Namespace : elastic Labels : <none> Annotations : eks.amazonaws.com/role-arn : arn:aws:iam::000000000000:role/eksctl-your-cluster-name-addon-iamserviceac-Role1-0A0A0A0A0A0A0 Image pull secrets : <none> Mountable secrets : fluent-bit-token-pgpss Tokens : fluent-bit-token-pgpss Events : <none> Now define the Elasticsearch domain This configuration will provision a public Elasticsearch cluster with Fine-Grained Access Control enabled and a built-in user database: cat <<EOF> ~/environment/logging/elasticsearch_domain.json { \"DomainName\": ${ES_DOMAIN_NAME}, \"ElasticsearchVersion\": ${ES_VERSION}, \"ElasticsearchClusterConfig\": { \"InstanceType\": \"r5.large.elasticsearch\", \"InstanceCount\": 1, \"DedicatedMasterEnabled\": false, \"ZoneAwarenessEnabled\": false, \"WarmEnabled\": false }, \"EBSOptions\": { \"EBSEnabled\": true, \"VolumeType\": \"gp2\", \"VolumeSize\": 100 }, \"AccessPolicies\": \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"AWS\\\":\\\"*\\\"},\\\"Action\\\":\\\"es:ESHttp*\\\",\\\"Resource\\\":\\\"arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}/*\\\"}]}\", \"SnapshotOptions\": {}, \"CognitoOptions\": { \"Enabled\": false }, \"EncryptionAtRestOptions\": { \"Enabled\": true }, \"NodeToNodeEncryptionOptions\": { \"Enabled\": true }, \"DomainEndpointOptions\": { \"EnforceHTTPS\": true, \"TLSSecurityPolicy\": \"Policy-Min-TLS-1-0-2019-07\" }, \"AdvancedSecurityOptions\": { \"Enabled\": true, \"InternalUserDatabaseEnabled\": true, \"MasterUserOptions\": { \"MasterUserName\": ${ES_USER}, \"MasterUserPassword\": ${ES_PASSWORD} } } } EOF Initialize the Elasticsearch domain using the elasticsearch_domain.json aws es create-elasticsearch-domain \\ --cli-input-json file://~/environment/logging/elasticsearch_domain.json It takes a while for Elasticsearch clusters to change to an active state. Check the AWS Console to see the status of the cluster, and continue to the next step when the cluster is ready. At this point you need to map roles to users in order to set fine-grained access control, because without this mapping all the requests to the cluster will result in permission errors. You should add the Fluentbit ARN as a backend role to the all-access role, which uses the Elasticsearch APIs. To find the fluentbit ARN run the following command and export the value of ARN Role into the FLUENTBIT_ROLE environment variable: eksctl get iamserviceaccount --cluster dcd-ap-southeast-2 The output of this command should look similar to this: NAMESPACE NAME ROLE ARN kube-system cluster-autoscaler arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-1RSRFV0BQVE3E Take note of the ROLE ARN and export it as the environment variable FLUENTBIT_ROLE export FLUENTBIT_ROLE = arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-1RSRFV0BQVE3E Retrieve the Elasticsearch endpoint and update the internal database: export ES_ENDPOINT = $( aws es describe-elasticsearch-domain --domain-name ngh-search-domain --output text --query \"DomainStatus.Endpoint\" ) curl -sS -u \" ${ ES_DOMAIN_USER } : ${ ES_DOMAIN_PASSWORD } \" \\ -X PATCH \\ https:// ${ ES_ENDPOINT } /_opendistro/_security/api/rolesmapping/all_access?pretty \\ -H 'Content-Type: application/json' \\ -d ' [ { \"op\": \"add\", \"path\": \"/backend_roles\", \"value\": [\"' ${ FLUENTBIT_ROLE } '\"] } ] ' Finally, it is time to deploy the Fluentbit DaemonSet: kubectl apply -f docs/docs/examples/logging/efk/managed_es/fluentbit.yaml After a few minutes all pods should be up and in running status. you can open Kibana to visualise the logs. The endpoint for Kibana can be found in the Elasticsearch output tab in the AWS console, or you can run the following command: echo \"Kibana URL: https:// ${ ES_ENDPOINT } /_plugin/kibana/\" Kibana URL: https://search-domain-uehlb3kxledxykchwexee.ap-southeast-2.es.amazonaws.com/_plugin/kibana/ The user and password for Kibana are the same as the master user credential that is set in Elasticsearch in the provisioning stage. Open Kibana in a browser and after login, create an index pattern and see the report in the Discover page.","title":"Logging in a Kubernetes environment"},{"location":"examples/logging/efk/EFK/#logging-in-a-kubernetes-environment","text":"Warning This functionality is not officially supported. This document explains how to enable aggregated logging in your Kubernetes cluster. There are many ways to do this and this document showcases only a few of the options.","title":"Logging in a Kubernetes environment"},{"location":"examples/logging/efk/EFK/#efk-stack","text":"A common Kubernetes logging pattern is the combination of Elasticsearch , Fluentd , and Kibana , known as EFK Stack . Fluentd is an open-source and multi-platform log processor that collects data/logs from different sources, aggregates, and forwards them to multiple destinations. It is fully compatible with Docker and Kubernetes environments. Elasticsearch is a distributed open search and analytics engine for all types of data. Kibana is an open-source front-end application that sits on top of Elasticsearch, providing search and data visualization capabilities for data indexed in Elasticsearch. There are different methods to deploy an EFK stack. We provide two deployment methods, the first is deploying EFK locally on Kubernetes, and the second is using a managed Elasticsearch instance outside the Kubernetes cluster.","title":"EFK stack"},{"location":"examples/logging/efk/EFK/#local-efk-stack","text":"This solution deploys the EFK stack inside the Kubernetes cluster. By setting fluentd.enabled value to true , Helm installs Fluentd on each of application pods. This means that after deployment all the product pods run Fluentd, which collects all the log files and sends them to the Fluentd aggregator container. To complete the EFK stack you need to install an Elasticsearch cluster and Kibana, and successfully forward the aggregated datalog to Elasticsearch using Fluentd, which is already installed. Follow these steps to install Elasticsearch","title":"Local EFK stack"},{"location":"examples/logging/efk/EFK/#1-install-elasticsearch","text":"Install Elasticsearch using the instructions documented here . Once installed make sure Elasticsearch cluster is working as expected by first port forwarding the service kubectl port-forward svc/elasticsearch-master 9200 you can then curl the endpoint for the current state $ curl localhost:9200 { \"name\" : \"elasticsearch-master-0\" , \"cluster_name\" : \"elasticsearch\" , \"cluster_uuid\" : \"uNdYC-2nSdWVdzPCw9P7jQ\" , \"version\" : { \"number\" : \"7.12.0\" , \"build_flavor\" : \"default\" , \"build_type\" : \"docker\" , \"build_hash\" : \"78722783c38caa25a70982b5b042074cde5d3b3a\" , \"build_date\" : \"2021-03-18T06:17:15.410153305Z\" , \"build_snapshot\" : false, \"lucene_version\" : \"8.8.0\" , \"minimum_wire_compatibility_version\" : \"6.8.0\" , \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" } , \"tagline\" : \"You Know, for Search\" }","title":"1. Install Elasticsearch"},{"location":"examples/logging/efk/EFK/#2-enable-fluentd","text":"Now enable Fluentd and set the hostname for Elasticsearch in values.yaml as follows: fluentd : enabled : true elasticsearch : hostname : elasticsearch-master Fluentd tries to parse and send the data to Elasticsearch, but since it's not installed the data is lost. At this point you have logged data in the installed Elasticsearch, and you should install Kibana to complete the EFK stack deployment:","title":"2. Enable Fluentd"},{"location":"examples/logging/efk/EFK/#3-install-kibana","text":"helm install kibana elastic/kibana Make sure kibana is running by checking the deployment kubectl get deployment You should see something like... NAME READY UP-TO-DATE AVAILABLE AGE helm-operator 1 /1 1 1 23m ingress-nginx-release-controller 1 /1 1 1 22m kibana-kibana 1 /1 1 1 25m Through port-forwarding you can access Kibana via http://localhost:5601 kubectl port-forward deployment/kibana-kibana 5601 To visualise the logs you need to create an index pattern and then look at the the data in the discovery part. To create the index pattern go to Management \u2192 Stack Management and then select Kibana \u2192 Index Patterns .","title":"3. Install Kibana"},{"location":"examples/logging/efk/EFK/#managed-efk-stack","text":"In this solution Elasticsearch is deployed as a managed AWS service and lives outside of the Kubernetes cluster. This approach uses Fluentbit instead of Fluentd for log processing. Fluentbit Fluentbit is used to collect and aggregate log data inside the EKS cluster. It then sends this to an AWS Elasticsearch instance outside of the cluster. When a node inside an EKS cluster needs to call an AWS API, it needs to provide extended permissions. Amazon provides an image of Fluentbit that supports AWS service accounts,and using this you no longer need to follow the traditional way. All you need is to have an IAM role for the AWS service account on an EKS cluster. Using this service account, an AWS permission can be provided to the containers in any pod that use that service account. The result is that the pods on that node can call AWS APIs. Your first step is to configure IAM roles for Service Accounts (IRSA) for Fluentbit , to make sure you have an OIDC identity provider to use IAM roles for the service account in the cluster: eksctl utils associate-iam-oidc-provider \\ --cluster <cluster_name> \\ --approve Then create an IAM policy to limit the permissions to connect to the Elasticsearch cluster. Before this, you need to set the following environment variables: Environment variable Value KUBE_NAMESPACE The namespace for kubernetes cluster ES_DOMAIN_NAME Elasticsearch domain name ES_VERSION Elasticsearch version ES_USER Elasticsearch username ES_PASSWORD Elasticsearch password (eg. export ES_PASSWORD=\"$(openssl rand -base64 8)_Ek1$\" ) ACCOUNT_ID AWS Account ID AWS_REGION AWS region code Now create the file fluent-bit-policy.json to define the policy itself: cat <<EoF > ~/environment/logging/fluent-bit-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"es:ESHttp*\" ], \"Resource\": \"arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}\", \"Effect\": \"Allow\" } ] } EoF Next initialize the policy: aws iam create-policy \\ --policy-name fluent-bit-policy \\ --policy-document file://~/environment/logging/fluent-bit-policy.json Create an IAM role for the service account: eksctl create iamserviceaccount \\ --name fluent-bit \\ --namespace dcd \\ --cluster dcd-ap-southeast-2 \\ --attach-policy-arn \"arn:aws:iam:: ${ ACCOUNT_ID } :policy/fluent-bit-policy\" \\ --approve \\ --override-existing-serviceaccounts Confirm that the service account with an Amazon Resource Name (ARN) of the IAM role is annotated: kubectl describe serviceaccount fluent-bit Look for output similar to: Name : fluent-bit Namespace : elastic Labels : <none> Annotations : eks.amazonaws.com/role-arn : arn:aws:iam::000000000000:role/eksctl-your-cluster-name-addon-iamserviceac-Role1-0A0A0A0A0A0A0 Image pull secrets : <none> Mountable secrets : fluent-bit-token-pgpss Tokens : fluent-bit-token-pgpss Events : <none> Now define the Elasticsearch domain This configuration will provision a public Elasticsearch cluster with Fine-Grained Access Control enabled and a built-in user database: cat <<EOF> ~/environment/logging/elasticsearch_domain.json { \"DomainName\": ${ES_DOMAIN_NAME}, \"ElasticsearchVersion\": ${ES_VERSION}, \"ElasticsearchClusterConfig\": { \"InstanceType\": \"r5.large.elasticsearch\", \"InstanceCount\": 1, \"DedicatedMasterEnabled\": false, \"ZoneAwarenessEnabled\": false, \"WarmEnabled\": false }, \"EBSOptions\": { \"EBSEnabled\": true, \"VolumeType\": \"gp2\", \"VolumeSize\": 100 }, \"AccessPolicies\": \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"AWS\\\":\\\"*\\\"},\\\"Action\\\":\\\"es:ESHttp*\\\",\\\"Resource\\\":\\\"arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}/*\\\"}]}\", \"SnapshotOptions\": {}, \"CognitoOptions\": { \"Enabled\": false }, \"EncryptionAtRestOptions\": { \"Enabled\": true }, \"NodeToNodeEncryptionOptions\": { \"Enabled\": true }, \"DomainEndpointOptions\": { \"EnforceHTTPS\": true, \"TLSSecurityPolicy\": \"Policy-Min-TLS-1-0-2019-07\" }, \"AdvancedSecurityOptions\": { \"Enabled\": true, \"InternalUserDatabaseEnabled\": true, \"MasterUserOptions\": { \"MasterUserName\": ${ES_USER}, \"MasterUserPassword\": ${ES_PASSWORD} } } } EOF Initialize the Elasticsearch domain using the elasticsearch_domain.json aws es create-elasticsearch-domain \\ --cli-input-json file://~/environment/logging/elasticsearch_domain.json It takes a while for Elasticsearch clusters to change to an active state. Check the AWS Console to see the status of the cluster, and continue to the next step when the cluster is ready. At this point you need to map roles to users in order to set fine-grained access control, because without this mapping all the requests to the cluster will result in permission errors. You should add the Fluentbit ARN as a backend role to the all-access role, which uses the Elasticsearch APIs. To find the fluentbit ARN run the following command and export the value of ARN Role into the FLUENTBIT_ROLE environment variable: eksctl get iamserviceaccount --cluster dcd-ap-southeast-2 The output of this command should look similar to this: NAMESPACE NAME ROLE ARN kube-system cluster-autoscaler arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-1RSRFV0BQVE3E Take note of the ROLE ARN and export it as the environment variable FLUENTBIT_ROLE export FLUENTBIT_ROLE = arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-1RSRFV0BQVE3E Retrieve the Elasticsearch endpoint and update the internal database: export ES_ENDPOINT = $( aws es describe-elasticsearch-domain --domain-name ngh-search-domain --output text --query \"DomainStatus.Endpoint\" ) curl -sS -u \" ${ ES_DOMAIN_USER } : ${ ES_DOMAIN_PASSWORD } \" \\ -X PATCH \\ https:// ${ ES_ENDPOINT } /_opendistro/_security/api/rolesmapping/all_access?pretty \\ -H 'Content-Type: application/json' \\ -d ' [ { \"op\": \"add\", \"path\": \"/backend_roles\", \"value\": [\"' ${ FLUENTBIT_ROLE } '\"] } ] ' Finally, it is time to deploy the Fluentbit DaemonSet: kubectl apply -f docs/docs/examples/logging/efk/managed_es/fluentbit.yaml After a few minutes all pods should be up and in running status. you can open Kibana to visualise the logs. The endpoint for Kibana can be found in the Elasticsearch output tab in the AWS console, or you can run the following command: echo \"Kibana URL: https:// ${ ES_ENDPOINT } /_plugin/kibana/\" Kibana URL: https://search-domain-uehlb3kxledxykchwexee.ap-southeast-2.es.amazonaws.com/_plugin/kibana/ The user and password for Kibana are the same as the master user credential that is set in Elasticsearch in the provisioning stage. Open Kibana in a browser and after login, create an index pattern and see the report in the Discover page.","title":"Managed EFK stack"},{"location":"examples/ssh/SSH_BITBUCKET/","text":"SSH service in Bitbucket on Kubernetes \u00b6 In addition to providing a service on HTTP(S), Bitbucket also allows remote Git operations over SSH connections. By default, Kubernetes Ingress controllers only work for HTTP connections, but some ingress controllers also support TCP connections. Depending on the need of your deployment, SSH access can be provided through two mechanisms: Opening the TCP port through the ingress controller - This option should be used if the SSH service is required to be available on the same DNS name as the HTTP service. Creating a separate Kubernetes LoadBalancer service - This option is available if the ingress controller does not support TCP connections, or if you don\u2019t need your deployment to have the SSH service available on the same DNS name as the HTTP service. NGINX Ingress controller config for SSH connections \u00b6 We can follow the official documentation for the NGINX Ingress controller for this: Exposing TCP and UDP services - NGINX Ingress Controller . Namespace co-location These instructions should be performed in the same namespace in which the Ingress controller resides. 1. Create ConfigMap \u00b6 Create a new ConfigMap : kubectl create configmap tcp-services In our example we deployed Bitbucket using the Helm release name bitbucket in the namespace ssh-test , update the ConfigMap tcp-services accordingly: apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx data : 7999 : \"ssh-test/bitbucket:ssh\" 2. Update Ingress deployment \u00b6 Next, we have to edit the deployment of the ingress controller and add the --tcp-services-configmap option: kubectl edit deployment <name of ingress-nginx deployment> Add this line in the args of the container spec : - --tcp-services-configmap = $( POD_NAMESPACE ) /tcp-services so it looks something like this: spec : containers : - args : - /nginx-ingress-controller - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller - --election-id=ingress-controller-leader - --ingress-class=nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services 3. Update the Ingress service \u00b6 Update the Ingress service to include an additional port definition for ssh kubectl edit service <name of ingress-nginx service> Add this section in the ports of the container spec : - name : ssh port : 7999 protocol : TCP so it looks something like this: spec : clusterIP : 10.100.19.60 externalTrafficPolicy : Cluster ports : - name : http nodePort : 31381 port : 80 protocol : TCP targetPort : http - name : https nodePort : 32612 port : 443 protocol : TCP targetPort : https - name : ssh port : 7999 protocol : TCP After the deployment has been upgraded, the SSH service should be available on port 7999 . LoadBalancer service for SSH connections on AWS \u00b6 In the values file for the helm chart, the extra SSH service can be enabled like this: bitbucket : sshService : enabled : true On a deployment using AWS, assuming you have external-dns configured, you can add these annotations to automatically set up the DNS name for the SSH service: bitbucket : sshService : enabled : true annotations : external-dns.alpha.kubernetes.io/hostname : bitbucket-ssh.example.com additionalEnvironmentVariables : - name : PLUGIN_SSH_BASEURL value : ssh://bitbucket-ssh.example.com/","title":"SSH service in Bitbucket on Kubernetes"},{"location":"examples/ssh/SSH_BITBUCKET/#ssh-service-in-bitbucket-on-kubernetes","text":"In addition to providing a service on HTTP(S), Bitbucket also allows remote Git operations over SSH connections. By default, Kubernetes Ingress controllers only work for HTTP connections, but some ingress controllers also support TCP connections. Depending on the need of your deployment, SSH access can be provided through two mechanisms: Opening the TCP port through the ingress controller - This option should be used if the SSH service is required to be available on the same DNS name as the HTTP service. Creating a separate Kubernetes LoadBalancer service - This option is available if the ingress controller does not support TCP connections, or if you don\u2019t need your deployment to have the SSH service available on the same DNS name as the HTTP service.","title":"SSH service in Bitbucket on Kubernetes"},{"location":"examples/ssh/SSH_BITBUCKET/#nginx-ingress-controller-config-for-ssh-connections","text":"We can follow the official documentation for the NGINX Ingress controller for this: Exposing TCP and UDP services - NGINX Ingress Controller . Namespace co-location These instructions should be performed in the same namespace in which the Ingress controller resides.","title":"NGINX Ingress controller config for SSH connections"},{"location":"examples/ssh/SSH_BITBUCKET/#1-create-configmap","text":"Create a new ConfigMap : kubectl create configmap tcp-services In our example we deployed Bitbucket using the Helm release name bitbucket in the namespace ssh-test , update the ConfigMap tcp-services accordingly: apiVersion : v1 kind : ConfigMap metadata : name : tcp-services namespace : ingress-nginx data : 7999 : \"ssh-test/bitbucket:ssh\"","title":"1. Create ConfigMap"},{"location":"examples/ssh/SSH_BITBUCKET/#2-update-ingress-deployment","text":"Next, we have to edit the deployment of the ingress controller and add the --tcp-services-configmap option: kubectl edit deployment <name of ingress-nginx deployment> Add this line in the args of the container spec : - --tcp-services-configmap = $( POD_NAMESPACE ) /tcp-services so it looks something like this: spec : containers : - args : - /nginx-ingress-controller - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller - --election-id=ingress-controller-leader - --ingress-class=nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services","title":"2. Update Ingress deployment"},{"location":"examples/ssh/SSH_BITBUCKET/#3-update-the-ingress-service","text":"Update the Ingress service to include an additional port definition for ssh kubectl edit service <name of ingress-nginx service> Add this section in the ports of the container spec : - name : ssh port : 7999 protocol : TCP so it looks something like this: spec : clusterIP : 10.100.19.60 externalTrafficPolicy : Cluster ports : - name : http nodePort : 31381 port : 80 protocol : TCP targetPort : http - name : https nodePort : 32612 port : 443 protocol : TCP targetPort : https - name : ssh port : 7999 protocol : TCP After the deployment has been upgraded, the SSH service should be available on port 7999 .","title":"3. Update the Ingress service"},{"location":"examples/ssh/SSH_BITBUCKET/#loadbalancer-service-for-ssh-connections-on-aws","text":"In the values file for the helm chart, the extra SSH service can be enabled like this: bitbucket : sshService : enabled : true On a deployment using AWS, assuming you have external-dns configured, you can add these annotations to automatically set up the DNS name for the SSH service: bitbucket : sshService : enabled : true annotations : external-dns.alpha.kubernetes.io/hostname : bitbucket-ssh.example.com additionalEnvironmentVariables : - name : PLUGIN_SSH_BASEURL value : ssh://bitbucket-ssh.example.com/","title":"LoadBalancer service for SSH connections on AWS"},{"location":"examples/storage/STORAGE/","text":"Shared storage \u00b6 Atlassian's Data Center products require a shared storage solution to effectively operate in multi-node environment. The specifics of how this shared storage is created is site-dependent, we do however provide examples on how shared storage can be created below. Due to the high requirements on performance for IO operations, Bitbucket needs a dedicated NFS server providing persistence for a shared home. See NFS example for details AWS EFS \u00b6 Jira, Confluence and Crowd can all be configured with an EFS-backed shared solution. For details on how this can be set up, see the AWS EFS example . NFS \u00b6 For details on creating shared storage for Bitbucket, see the NFS example .","title":"Shared storage"},{"location":"examples/storage/STORAGE/#shared-storage","text":"Atlassian's Data Center products require a shared storage solution to effectively operate in multi-node environment. The specifics of how this shared storage is created is site-dependent, we do however provide examples on how shared storage can be created below. Due to the high requirements on performance for IO operations, Bitbucket needs a dedicated NFS server providing persistence for a shared home. See NFS example for details","title":"Shared storage"},{"location":"examples/storage/STORAGE/#aws-efs","text":"Jira, Confluence and Crowd can all be configured with an EFS-backed shared solution. For details on how this can be set up, see the AWS EFS example .","title":"AWS EFS"},{"location":"examples/storage/STORAGE/#nfs","text":"For details on creating shared storage for Bitbucket, see the NFS example .","title":"NFS"},{"location":"examples/storage/aws/LOCAL_STORAGE/","text":"Local storage \u00b6 This file provides examples on how a Kubernetes cluster and helm deployment can be configured to utilize AWS EBS backed volumes. Dynamic provisioning \u00b6 Due to the ephemeral nature of Kubernetes pods we advise dynamic provisioning be used for creating and consuming EBS volume(s). Prerequisites \u00b6 Ensure the EBS CSI driver is installed within the k8s cluster, you can confirm this by running: kubectl get csidriver the output of the above command should include the named driver ebs.csi.aws.com for example: NAME ATTACHREQUIRED PODINFOONMOUNT MODES AGE ebs.csi.aws.com true false Persistent 5d1h If not present the EBS driver can be installed using the following instructions here . Provisioning \u00b6 Create a Storage Class Update values.yaml to utilise Storage Class 1. Create Storage Class \u00b6 kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : ebs-sc reclaimPolicy : Retain provisioner : ebs.csi.aws.com volumeBindingMode : WaitForFirstConsumer 2. Update values.yaml \u00b6 Update the localHome storageClassName value within values.yaml to the name of the Storage Class created in step 1 above volumes : localHome : persistentVolumeClaim : create : true storageClassName : \"ebs-sc\" Resources \u00b6 Some useful resources on provisioning local storage with the AWS CSI Driver EBS CSI driver - GitHub Repo Official Amazon EBS CSI driver documentation Product installation Creating the local home volume is the final step in provisioning the required infrastructure . You can now move onto the next step, Installation .","title":"Local storage"},{"location":"examples/storage/aws/LOCAL_STORAGE/#local-storage","text":"This file provides examples on how a Kubernetes cluster and helm deployment can be configured to utilize AWS EBS backed volumes.","title":"Local storage"},{"location":"examples/storage/aws/LOCAL_STORAGE/#dynamic-provisioning","text":"Due to the ephemeral nature of Kubernetes pods we advise dynamic provisioning be used for creating and consuming EBS volume(s).","title":"Dynamic provisioning"},{"location":"examples/storage/aws/LOCAL_STORAGE/#prerequisites","text":"Ensure the EBS CSI driver is installed within the k8s cluster, you can confirm this by running: kubectl get csidriver the output of the above command should include the named driver ebs.csi.aws.com for example: NAME ATTACHREQUIRED PODINFOONMOUNT MODES AGE ebs.csi.aws.com true false Persistent 5d1h If not present the EBS driver can be installed using the following instructions here .","title":"Prerequisites"},{"location":"examples/storage/aws/LOCAL_STORAGE/#provisioning","text":"Create a Storage Class Update values.yaml to utilise Storage Class","title":"Provisioning"},{"location":"examples/storage/aws/LOCAL_STORAGE/#1-create-storage-class","text":"kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : ebs-sc reclaimPolicy : Retain provisioner : ebs.csi.aws.com volumeBindingMode : WaitForFirstConsumer","title":"1. Create Storage Class"},{"location":"examples/storage/aws/LOCAL_STORAGE/#2-update-valuesyaml","text":"Update the localHome storageClassName value within values.yaml to the name of the Storage Class created in step 1 above volumes : localHome : persistentVolumeClaim : create : true storageClassName : \"ebs-sc\"","title":"2. Update values.yaml"},{"location":"examples/storage/aws/LOCAL_STORAGE/#resources","text":"Some useful resources on provisioning local storage with the AWS CSI Driver EBS CSI driver - GitHub Repo Official Amazon EBS CSI driver documentation Product installation Creating the local home volume is the final step in provisioning the required infrastructure . You can now move onto the next step, Installation .","title":"Resources"},{"location":"examples/storage/aws/SHARED_STORAGE/","text":"Shared storage \u00b6 This file provides examples on how a Kubernetes cluster and helm deployment can be configured to utilize an AWS EFS backed filesystem. Static provisioning \u00b6 An example detailing how an existing EFS filesystem can be created and consumed using static provisioning. Prerequisites \u00b6 EFS CSI driver is installed within the k8s cluster. A physical EFS filesystem has been provisioned Additional details on static EFS provisioning can be found here You can confirm that the EFS CSI driver has been installed by running: kubectl get csidriver the output of the above command should include the named driver efs.csi.aws.com for example: NAME ATTACHREQUIRED PODINFOONMOUNT MODES AGE efs.csi.aws.com false false Persistent 5d3h Provisioning \u00b6 Create a Persistent Volume Create a Persistent Volume Claim Update values.yaml to utilise Persistent Volume Claim 1. Create Persistent Volume \u00b6 Create a persistent volume for the pre-provisioned EFS filesystem by providing the <efs-id> . The EFS id can be identified using the CLI command below with the appropriate region aws efs describe-file-systems --query \"FileSystems[*].FileSystemId\" --region ap-southeast-2 apiVersion : v1 kind : PersistentVolume metadata : name : my-shared-vol-pv spec : capacity : storage : 1Gi volumeMode : Filesystem accessModes : - ReadWriteMany storageClassName : efs-pv persistentVolumeReclaimPolicy : Retain mountOptions : - rw - lookupcache=pos - noatime - intr - _netdev csi : driver : efs.csi.aws.com volumeHandle : <efs-id> 2. Create Persistent Volume Claim \u00b6 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : my-shared-vol-pvc spec : accessModes : - ReadWriteMany storageClassName : efs-pv volumeMode : Filesystem volumeName : my-shared-vol-pv resources : requests : storage : 1Gi 3. Update values.yaml \u00b6 Update the sharedHome claimName value within values.yaml to the name of the Persistent Volume Claim created in step 2 above volumes : sharedHome : customVolume : persistentVolumeClaim : claimName : \"my-shared-vol-pvc\" Resources \u00b6 Some useful resources on provisioning shared storage with the AWS CSI Driver: Amazon EFS CSI driver Introducing Amazon EFS CSI dynamic provisioning Next step - Local storage Having created the shared home volume continue with provisioning the next piece of prerequisite infrastructure, local storage .","title":"Shared storage"},{"location":"examples/storage/aws/SHARED_STORAGE/#shared-storage","text":"This file provides examples on how a Kubernetes cluster and helm deployment can be configured to utilize an AWS EFS backed filesystem.","title":"Shared storage"},{"location":"examples/storage/aws/SHARED_STORAGE/#static-provisioning","text":"An example detailing how an existing EFS filesystem can be created and consumed using static provisioning.","title":"Static provisioning"},{"location":"examples/storage/aws/SHARED_STORAGE/#prerequisites","text":"EFS CSI driver is installed within the k8s cluster. A physical EFS filesystem has been provisioned Additional details on static EFS provisioning can be found here You can confirm that the EFS CSI driver has been installed by running: kubectl get csidriver the output of the above command should include the named driver efs.csi.aws.com for example: NAME ATTACHREQUIRED PODINFOONMOUNT MODES AGE efs.csi.aws.com false false Persistent 5d3h","title":"Prerequisites"},{"location":"examples/storage/aws/SHARED_STORAGE/#provisioning","text":"Create a Persistent Volume Create a Persistent Volume Claim Update values.yaml to utilise Persistent Volume Claim","title":"Provisioning"},{"location":"examples/storage/aws/SHARED_STORAGE/#1-create-persistent-volume","text":"Create a persistent volume for the pre-provisioned EFS filesystem by providing the <efs-id> . The EFS id can be identified using the CLI command below with the appropriate region aws efs describe-file-systems --query \"FileSystems[*].FileSystemId\" --region ap-southeast-2 apiVersion : v1 kind : PersistentVolume metadata : name : my-shared-vol-pv spec : capacity : storage : 1Gi volumeMode : Filesystem accessModes : - ReadWriteMany storageClassName : efs-pv persistentVolumeReclaimPolicy : Retain mountOptions : - rw - lookupcache=pos - noatime - intr - _netdev csi : driver : efs.csi.aws.com volumeHandle : <efs-id>","title":"1. Create Persistent Volume"},{"location":"examples/storage/aws/SHARED_STORAGE/#2-create-persistent-volume-claim","text":"apiVersion : v1 kind : PersistentVolumeClaim metadata : name : my-shared-vol-pvc spec : accessModes : - ReadWriteMany storageClassName : efs-pv volumeMode : Filesystem volumeName : my-shared-vol-pv resources : requests : storage : 1Gi","title":"2. Create Persistent Volume Claim"},{"location":"examples/storage/aws/SHARED_STORAGE/#3-update-valuesyaml","text":"Update the sharedHome claimName value within values.yaml to the name of the Persistent Volume Claim created in step 2 above volumes : sharedHome : customVolume : persistentVolumeClaim : claimName : \"my-shared-vol-pvc\"","title":"3. Update values.yaml"},{"location":"examples/storage/aws/SHARED_STORAGE/#resources","text":"Some useful resources on provisioning shared storage with the AWS CSI Driver: Amazon EFS CSI driver Introducing Amazon EFS CSI dynamic provisioning Next step - Local storage Having created the shared home volume continue with provisioning the next piece of prerequisite infrastructure, local storage .","title":"Resources"},{"location":"examples/storage/nfs/NFS/","text":"NFS server for Bitbucket \u00b6 Disclaimer This functionality is not officially supported. It should not be used for production deployments! The included NFS example is provided as is and should be used as reference a only. Before you proceed we highly recommend that you understand your specific deployment needs and tailor your solution to them. Bitbucket Data Center and NFS \u00b6 Due to the high performance requirements on IO operations, Bitbucket needs a dedicated NFS server providing persistence for a shared home. For this reason we don't recommend that you use cloud managed storage services such as AWS EFS. NFS provisioning \u00b6 The NFS server can be provisioned manually or by using the supplied Helm chart. Details for both approaches can be found below. Pod affinity To reduce the IO latency between the NFS server and Bitbucket Pod(s) it is highly recommend to keep them in close proximity. To achieve this, you can use standard Kubernetes affinity rules . The affinity stanza within values.yaml can be updated to take advantage of this behaviour i.e. Manual \u00b6 For information on setting up Bitbucket Data Center's shared file server, see Provision your shared file system . This section contains the requirements and recommendations for setting up NFS for Bitbucket Data Center. NFS Server sizing Ensure the NFS server's size is appropriate for the needs of the Bitbucket instance. See capacity recommendations for details. Helm \u00b6 Disclaimer This Helm chart is not officially supported! It should not be used for production deployments! Installation \u00b6 Create a namespace for the NFS kubectl create namespace nfs Clone this repository and from the sub-directory, data-center-helm-charts/docs/docs/examples/storage/nfs , run the following command: helm install nfs-server nfs-server-example --namespace nfs Uninstall \u00b6 helm uninstall nfs-server --namespace nfs Update values.yaml \u00b6 Get the IP address of the NFS service ( CLUSTER-IP ) by running the following command kubectl get service --namespace nfs -o jsonpath = '{.items[0].spec.clusterIP}' NFS directory share The NFS Helm chart creates and exposes the directory share /srv/nfs . This will be required when configuring values.yaml The approach below shows how a persistentVolume and corresponding peristentVolumeClaim can be dynamically created for the provisioned NFS. Using the NFS IP and directory share, (see above) update the values.yaml appropriately: volumes : sharedHome : persistentVolume : create : true nfs : server : \"10.100.197.23\" # IP address of the NFS server path : \"/srv/nfs\" # Directory share of NFS persistentVolumeClaim : create : true storageClassName : \"\" You can of course manually provision your own persistentVolume and corresponding claim (as opposed to the dynamic approach described above) for the NFS server. In this case update the values.yaml to make use of them via the customVolume stanza, sharedHome.persistentVolume.create and sharedHome.persistentVolumeClaim.create should also both be set to false . sharedHome : persistentVolume : create : false persistentVolumeClaim : create : false customVolume : persistentVolumeClaim : claimName : \"custom-nfs-server-claim\" Next step - Local storage Having created the shared home NFS continue with provisioning the next piece of prerequisite infrastructure, local storage .","title":"NFS server for Bitbucket"},{"location":"examples/storage/nfs/NFS/#nfs-server-for-bitbucket","text":"Disclaimer This functionality is not officially supported. It should not be used for production deployments! The included NFS example is provided as is and should be used as reference a only. Before you proceed we highly recommend that you understand your specific deployment needs and tailor your solution to them.","title":"NFS server for Bitbucket"},{"location":"examples/storage/nfs/NFS/#bitbucket-data-center-and-nfs","text":"Due to the high performance requirements on IO operations, Bitbucket needs a dedicated NFS server providing persistence for a shared home. For this reason we don't recommend that you use cloud managed storage services such as AWS EFS.","title":"Bitbucket Data Center and NFS"},{"location":"examples/storage/nfs/NFS/#nfs-provisioning","text":"The NFS server can be provisioned manually or by using the supplied Helm chart. Details for both approaches can be found below. Pod affinity To reduce the IO latency between the NFS server and Bitbucket Pod(s) it is highly recommend to keep them in close proximity. To achieve this, you can use standard Kubernetes affinity rules . The affinity stanza within values.yaml can be updated to take advantage of this behaviour i.e.","title":"NFS provisioning"},{"location":"examples/storage/nfs/NFS/#manual","text":"For information on setting up Bitbucket Data Center's shared file server, see Provision your shared file system . This section contains the requirements and recommendations for setting up NFS for Bitbucket Data Center. NFS Server sizing Ensure the NFS server's size is appropriate for the needs of the Bitbucket instance. See capacity recommendations for details.","title":"Manual"},{"location":"examples/storage/nfs/NFS/#helm","text":"Disclaimer This Helm chart is not officially supported! It should not be used for production deployments!","title":"Helm"},{"location":"examples/storage/nfs/NFS/#installation","text":"Create a namespace for the NFS kubectl create namespace nfs Clone this repository and from the sub-directory, data-center-helm-charts/docs/docs/examples/storage/nfs , run the following command: helm install nfs-server nfs-server-example --namespace nfs","title":"Installation"},{"location":"examples/storage/nfs/NFS/#uninstall","text":"helm uninstall nfs-server --namespace nfs","title":"Uninstall"},{"location":"examples/storage/nfs/NFS/#update-valuesyaml","text":"Get the IP address of the NFS service ( CLUSTER-IP ) by running the following command kubectl get service --namespace nfs -o jsonpath = '{.items[0].spec.clusterIP}' NFS directory share The NFS Helm chart creates and exposes the directory share /srv/nfs . This will be required when configuring values.yaml The approach below shows how a persistentVolume and corresponding peristentVolumeClaim can be dynamically created for the provisioned NFS. Using the NFS IP and directory share, (see above) update the values.yaml appropriately: volumes : sharedHome : persistentVolume : create : true nfs : server : \"10.100.197.23\" # IP address of the NFS server path : \"/srv/nfs\" # Directory share of NFS persistentVolumeClaim : create : true storageClassName : \"\" You can of course manually provision your own persistentVolume and corresponding claim (as opposed to the dynamic approach described above) for the NFS server. In this case update the values.yaml to make use of them via the customVolume stanza, sharedHome.persistentVolume.create and sharedHome.persistentVolumeClaim.create should also both be set to false . sharedHome : persistentVolume : create : false persistentVolumeClaim : create : false customVolume : persistentVolumeClaim : claimName : \"custom-nfs-server-claim\" Next step - Local storage Having created the shared home NFS continue with provisioning the next piece of prerequisite infrastructure, local storage .","title":"Update values.yaml"},{"location":"platforms/OPENSHIFT/","text":"OpenShift \u00b6 The Helm charts are vendor agnostic and create objects from standard APIs that OpenShift fully supports. However, by default OpenShift will not allow running containers as users specified in the image Dockerfiles or securityContext.fsGroup in a statefulset/deployment spec. There are a couple of ways to fix this. Attach anyuid policies \u00b6 If possible, attach anyuid policy to 2 serviceAccounts. Here's an example for a Bitbucket installation. Please, note that the service account names vary depending on the Data Center product: For Bitbucket pods oc adm policy add-scc-to-user anyuid -z bitbucket -n git For NFS permission fixer pod oc adm policy add-scc-to-user anyuid -z default -n git Typically, the volumes.sharedHome.persistentVolumeClaim.nfsPermissionFixer needs to be set to true to make volume writable. It depends on the storage backend though. Set no security context \u00b6 As an alternative, (if letting containers run as pre-defined users is not possible), set product_name.securityContext.enabled to false . As a result the container will start as a user with an OpenShift generated ID. Typically, NFS permission fixer job isn't required when no security context is set. OpenShift Routes \u00b6 The Helm charts do not have templates for OpenShift routes that are commonly used in OpenShift instead of ingresses. Routes need to be manually created after the charts installation.","title":"OpenShift"},{"location":"platforms/OPENSHIFT/#openshift","text":"The Helm charts are vendor agnostic and create objects from standard APIs that OpenShift fully supports. However, by default OpenShift will not allow running containers as users specified in the image Dockerfiles or securityContext.fsGroup in a statefulset/deployment spec. There are a couple of ways to fix this.","title":"OpenShift"},{"location":"platforms/OPENSHIFT/#attach-anyuid-policies","text":"If possible, attach anyuid policy to 2 serviceAccounts. Here's an example for a Bitbucket installation. Please, note that the service account names vary depending on the Data Center product: For Bitbucket pods oc adm policy add-scc-to-user anyuid -z bitbucket -n git For NFS permission fixer pod oc adm policy add-scc-to-user anyuid -z default -n git Typically, the volumes.sharedHome.persistentVolumeClaim.nfsPermissionFixer needs to be set to true to make volume writable. It depends on the storage backend though.","title":"Attach anyuid policies"},{"location":"platforms/OPENSHIFT/#set-no-security-context","text":"As an alternative, (if letting containers run as pre-defined users is not possible), set product_name.securityContext.enabled to false . As a result the container will start as a user with an OpenShift generated ID. Typically, NFS permission fixer job isn't required when no security context is set.","title":"Set no security context"},{"location":"platforms/OPENSHIFT/#openshift-routes","text":"The Helm charts do not have templates for OpenShift routes that are commonly used in OpenShift instead of ingresses. Routes need to be manually created after the charts installation.","title":"OpenShift Routes"},{"location":"platforms/PLATFORMS/","text":"Platform information \u00b6 Support disclaimer The platforms documented on this page are not officially supported as part of the Atlassian Data Center products, and should be used only as examples. Using our Helm charts on different platforms: OpenShift","title":"Platform information"},{"location":"platforms/PLATFORMS/#platform-information","text":"Support disclaimer The platforms documented on this page are not officially supported as part of the Atlassian Data Center products, and should be used only as examples. Using our Helm charts on different platforms: OpenShift","title":"Platform information"},{"location":"troubleshooting/LIMITATIONS/","text":"Limitations \u00b6 Product limitations \u00b6 We haven't changed our Data Center applications' architecture to support Kubernetes. So, as is with all our Data Center products, the following limitiations still exist: We don't support horizontal or vertical autoscaling in our products. Read about Product scaling . More pods doesn\u2019t mean that the application will be more performant. We still have session affinity, so you will need to have a network setup that supports that. Jira and horizontal scaling \u00b6 At present there are issues relating to index replication with Jira when immediately scaling up by more than 1 pod at a time. Index replication service is paused indefinitely Automatic restore of indexes will fail Indexing improvements Please note that Jira is actively being worked on to address these issues in the coming releases. Although these issues are Jira specific, they are exasperated on account of the significantly reduced startup times for Jira when running in a Kubernetes cluster. As such these issues can have an impact on horizontal scaling if you don't take the correct approach . Platform limitations \u00b6 These configurations are explicitly not supported and the Helm charts don\u2019t work without modifications in these environments: Istio infrastructure Due to several reasons, Istio is imposing networking rules on every workload in the Kubernetes cluster that doesn\u2019t work with our deployments. The current recommendation is to create an exemption for our workloads if Istio is enabled in the cluster by default. Air-tight network (no outgoing requests) Some of our components are installed from publicly available repositories. When they can't reach the internet, they won\u2019t work.","title":"Limitations"},{"location":"troubleshooting/LIMITATIONS/#limitations","text":"","title":"Limitations"},{"location":"troubleshooting/LIMITATIONS/#product-limitations","text":"We haven't changed our Data Center applications' architecture to support Kubernetes. So, as is with all our Data Center products, the following limitiations still exist: We don't support horizontal or vertical autoscaling in our products. Read about Product scaling . More pods doesn\u2019t mean that the application will be more performant. We still have session affinity, so you will need to have a network setup that supports that.","title":"Product limitations"},{"location":"troubleshooting/LIMITATIONS/#jira-and-horizontal-scaling","text":"At present there are issues relating to index replication with Jira when immediately scaling up by more than 1 pod at a time. Index replication service is paused indefinitely Automatic restore of indexes will fail Indexing improvements Please note that Jira is actively being worked on to address these issues in the coming releases. Although these issues are Jira specific, they are exasperated on account of the significantly reduced startup times for Jira when running in a Kubernetes cluster. As such these issues can have an impact on horizontal scaling if you don't take the correct approach .","title":"Jira and horizontal scaling"},{"location":"troubleshooting/LIMITATIONS/#platform-limitations","text":"These configurations are explicitly not supported and the Helm charts don\u2019t work without modifications in these environments: Istio infrastructure Due to several reasons, Istio is imposing networking rules on every workload in the Kubernetes cluster that doesn\u2019t work with our deployments. The current recommendation is to create an exemption for our workloads if Istio is enabled in the cluster by default. Air-tight network (no outgoing requests) Some of our components are installed from publicly available repositories. When they can't reach the internet, they won\u2019t work.","title":"Platform limitations"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/","text":"Support boundaries \u00b6 This page describes what is within our scope of support for Kubernetes deployments, and what isn't. Additional information Read our troubleshooting tips . Read about the product and platform limitations . Supported components \u00b6 Helm charts \u00b6 Helm is a Kubernetes package manager. It allows us to provide generic YAML templates that you configure with the specific values for your environments. As described in the Prerequisites , you are responsible for creating the components that are required by the product for your type of deployment. You need to supply the appropriate values to your specific values.yaml file that is used for installation. We provide documentation for different configuration options in the Configuration guide and the Examples . If you have followed our documentation on how to configure the Helm charts, and you're using correctly created components, we will then provide support if you encounter an error with installation post helm install . If you find any issues, raise a ticket with our support team . If you have general feedback or questions regarding the charts, use Atlassian Community Kubernetes space . Unsupported components \u00b6 The Prerequisites can be created in multiple ways. You are responsible for creating them correctly so that hey can be used successfully with the Helm charts. Additional details on these prerequisites and their requirements below: Kubernetes cluster \u00b6 You need to make sure that you have enough privileges to run the application and create all the necessary entities that the Helm charts require. There are also different Kubernetes flavours that might require specific knowledge of how to install the products in them. For example, OpenShift and Rancher have more strict rules regarding container permissions. See examples of provisioning Kubernetes clusters on cloud-based providers . Shared storage \u00b6 Kubernetes setup requires you to have shared storage if you want to have a clustered instance. It's completely up to you how you set up the shared storage. The main requirement is that this storage needs to be accessible from Kubernetes and needs to be accessible from multiple pods. You can use a managed storage solution like EFS, Azure files, or some other dedicated solution that provides NFS-like access (e.g. dedicated NFS server, NetApp). There is a large number of combinations and potential setup scenarios and we can't support all of them. Our Helm charts expect you to provide a persistent volume claim, or a similar accessible shared storage in the values.yaml file. See examples of creating shared storage . For more information about volumes go to the Volumes section of the configuration guide . Networking \u00b6 You're required to configure the network access to the cluster. In Kubernetes, this usually means providing an ingress controller. It is up to you to make sure that the network configuration doesn\u2019t prevent nodes from communicating with each other and other components. You also need to make sure that your instance is accessible to the users (DNS, firewalls, VPC config). See an example of provisioning an NGINX Ingress controller . Database \u00b6 Through the values.yaml the database is provided as a connection string with the option to provide credentials and a driver. The database needs to be configured following product-specific requirements and needs to be accessible from Kubernetes. See an example of provisioning databases on cloud-based providers .","title":"Support boundaries"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#support-boundaries","text":"This page describes what is within our scope of support for Kubernetes deployments, and what isn't. Additional information Read our troubleshooting tips . Read about the product and platform limitations .","title":"Support boundaries"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#supported-components","text":"","title":"Supported components"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#helm-charts","text":"Helm is a Kubernetes package manager. It allows us to provide generic YAML templates that you configure with the specific values for your environments. As described in the Prerequisites , you are responsible for creating the components that are required by the product for your type of deployment. You need to supply the appropriate values to your specific values.yaml file that is used for installation. We provide documentation for different configuration options in the Configuration guide and the Examples . If you have followed our documentation on how to configure the Helm charts, and you're using correctly created components, we will then provide support if you encounter an error with installation post helm install . If you find any issues, raise a ticket with our support team . If you have general feedback or questions regarding the charts, use Atlassian Community Kubernetes space .","title":"Helm charts"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#unsupported-components","text":"The Prerequisites can be created in multiple ways. You are responsible for creating them correctly so that hey can be used successfully with the Helm charts. Additional details on these prerequisites and their requirements below:","title":"Unsupported components"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#kubernetes-cluster","text":"You need to make sure that you have enough privileges to run the application and create all the necessary entities that the Helm charts require. There are also different Kubernetes flavours that might require specific knowledge of how to install the products in them. For example, OpenShift and Rancher have more strict rules regarding container permissions. See examples of provisioning Kubernetes clusters on cloud-based providers .","title":"Kubernetes cluster"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#shared-storage","text":"Kubernetes setup requires you to have shared storage if you want to have a clustered instance. It's completely up to you how you set up the shared storage. The main requirement is that this storage needs to be accessible from Kubernetes and needs to be accessible from multiple pods. You can use a managed storage solution like EFS, Azure files, or some other dedicated solution that provides NFS-like access (e.g. dedicated NFS server, NetApp). There is a large number of combinations and potential setup scenarios and we can't support all of them. Our Helm charts expect you to provide a persistent volume claim, or a similar accessible shared storage in the values.yaml file. See examples of creating shared storage . For more information about volumes go to the Volumes section of the configuration guide .","title":"Shared storage"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#networking","text":"You're required to configure the network access to the cluster. In Kubernetes, this usually means providing an ingress controller. It is up to you to make sure that the network configuration doesn\u2019t prevent nodes from communicating with each other and other components. You also need to make sure that your instance is accessible to the users (DNS, firewalls, VPC config). See an example of provisioning an NGINX Ingress controller .","title":"Networking"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#database","text":"Through the values.yaml the database is provided as a connection string with the option to provide credentials and a driver. The database needs to be configured following product-specific requirements and needs to be accessible from Kubernetes. See an example of provisioning databases on cloud-based providers .","title":"Database"},{"location":"troubleshooting/TROUBLESHOOTING/","text":"Troubleshooting tips \u00b6 This guide contains general tips on how to investigate an application deployment that doesn't work correctly. General tips \u00b6 First, it is important to gather the information that will help you better understand where to focus your investigation efforts. The next section assumes you've followed the installation and configuration guides, and you can't access the installed product service. For installation troubleshooting, you will need to access the Kubernetes cluster and have enough permissions to follow the commands below. We highly recommend that you read through the Kubernetes official documentation describing monitoring, logging and debugging . Additionally, for great starting tips read the Application Introspection and Debugging section . Value placeholders Some commands include <release_name> and <namespace> . Replace them with the Helm release name and namespace specified when running helm install . My service is not accessible \u00b6 After helm install finishes, it prints a product service URL. It usually takes a few minutes for the service to start. If you visit the URL too soon, it might return a 5XX error HTTP code (the actual code might be dependent on your network implementation). If you have waited long enough (more than 10 minutes), and the service is still not accessible, it is time to investigate the reason why this is the case. Helm release verification \u00b6 Run helm list --all-namespaces to get the list of all installed chart releases. You should be able to see your installation in the list The status for the release should be deployed Run helm test <release_name> -n <namespace> This should return application tests in succeeded phase In case there are any test failures you will need to further investigate the particular domain DNS verification \u00b6 To verify the DNS record from your machine, run a basic dig test: dig SERVICE_DOMAIN_NAME Or use a web version of the tool . Investigate application logs \u00b6 You can get application logs from the pods with a standard kubectl command: kubectl logs APPLICATION_POD_NAME You can read the output and make sure it doesn't contain an error or an exception. Get application pod details \u00b6 For more details follow the official guide for debugging . Get the list of pods and their states: kubectl get <release_name> -n <namespace> -o wide Get details about a specific pod: kubectl describe POD_NAME -n <namespace> Get storage details \u00b6 Each application pod needs to have successfully mounted local and shared home. You can find out the details for the persistent volume claims with this command: Prerequisities The example needs to have jq tool installed. kubectl get pods --all-namespaces -o = json | jq -c \\ '.items[] | {name: .metadata.name, namespace: .metadata.namespace, claimName:.spec.volumes[] | select( has (\"persistentVolumeClaim\") ).persistentVolumeClaim.claimName }' Find all the application pods in the output and verify they have the correct claims (shared home and local home). For more details follow the documentation for persistent volumes .","title":"Troubleshooting tips"},{"location":"troubleshooting/TROUBLESHOOTING/#troubleshooting-tips","text":"This guide contains general tips on how to investigate an application deployment that doesn't work correctly.","title":"Troubleshooting tips"},{"location":"troubleshooting/TROUBLESHOOTING/#general-tips","text":"First, it is important to gather the information that will help you better understand where to focus your investigation efforts. The next section assumes you've followed the installation and configuration guides, and you can't access the installed product service. For installation troubleshooting, you will need to access the Kubernetes cluster and have enough permissions to follow the commands below. We highly recommend that you read through the Kubernetes official documentation describing monitoring, logging and debugging . Additionally, for great starting tips read the Application Introspection and Debugging section . Value placeholders Some commands include <release_name> and <namespace> . Replace them with the Helm release name and namespace specified when running helm install .","title":"General tips"},{"location":"troubleshooting/TROUBLESHOOTING/#my-service-is-not-accessible","text":"After helm install finishes, it prints a product service URL. It usually takes a few minutes for the service to start. If you visit the URL too soon, it might return a 5XX error HTTP code (the actual code might be dependent on your network implementation). If you have waited long enough (more than 10 minutes), and the service is still not accessible, it is time to investigate the reason why this is the case.","title":"My service is not accessible"},{"location":"troubleshooting/TROUBLESHOOTING/#helm-release-verification","text":"Run helm list --all-namespaces to get the list of all installed chart releases. You should be able to see your installation in the list The status for the release should be deployed Run helm test <release_name> -n <namespace> This should return application tests in succeeded phase In case there are any test failures you will need to further investigate the particular domain","title":"Helm release verification"},{"location":"troubleshooting/TROUBLESHOOTING/#dns-verification","text":"To verify the DNS record from your machine, run a basic dig test: dig SERVICE_DOMAIN_NAME Or use a web version of the tool .","title":"DNS verification"},{"location":"troubleshooting/TROUBLESHOOTING/#investigate-application-logs","text":"You can get application logs from the pods with a standard kubectl command: kubectl logs APPLICATION_POD_NAME You can read the output and make sure it doesn't contain an error or an exception.","title":"Investigate application logs"},{"location":"troubleshooting/TROUBLESHOOTING/#get-application-pod-details","text":"For more details follow the official guide for debugging . Get the list of pods and their states: kubectl get <release_name> -n <namespace> -o wide Get details about a specific pod: kubectl describe POD_NAME -n <namespace>","title":"Get application pod details"},{"location":"troubleshooting/TROUBLESHOOTING/#get-storage-details","text":"Each application pod needs to have successfully mounted local and shared home. You can find out the details for the persistent volume claims with this command: Prerequisities The example needs to have jq tool installed. kubectl get pods --all-namespaces -o = json | jq -c \\ '.items[] | {name: .metadata.name, namespace: .metadata.namespace, claimName:.spec.volumes[] | select( has (\"persistentVolumeClaim\") ).persistentVolumeClaim.claimName }' Find all the application pods in the output and verify they have the correct claims (shared home and local home). For more details follow the documentation for persistent volumes .","title":"Get storage details"},{"location":"userguide/CONFIGURATION/","text":"Configuration \u00b6 Ingress \u00b6 In order to make the Atlassian product available from outside of the Kubernetes cluster, a suitable HTTP/HTTPS ingress controller needs to be installed. The standard Kubernetes Ingress resource is not flexible enough for our needs, so a third-party ingress controller and resource definition must be provided. The exact details of the Ingress will be highly site-specific. These Helm charts were tested using the NGINX Ingress Controller . We also provide example instructions on how this controller can be installed and configured. The charts themselves provide a template for Ingress resource rules to be utilised by the provisioned controller. These include all required annotations and optional TLS configuration for the NGINX Ingress Controller. Some key considerations to note when configuring the controller are: Ingress At a minimum, the ingress needs the ability to support long request timeouts, as well as session affinity (aka \"sticky sessions\"). The Ingress Resource provided as part of the Helm charts is geared toward the NGINX Ingress Controller and can be configured via the ingress stanza in the appropriate values.yaml . Some key aspects that can be configured include: Usage of the NGINX Ingress Controller Ingress Controller annotations The request max body size The hostname of the ingress resource When installed, with the provided configuration , the NGINX Ingress Controller will provision an internet-facing (see diagram below) load balancer on your behalf. The load balancer should either support the Proxy Protocol or allow for the forwarding of X-Forwarded-* headers. This ensures any backend redirects are done so over the correct protocol. If the X-Forwarded-* headers are being used, then enable the use-forwarded-headers option on the controllers ConfigMap . This ensures that these headers are appropriately passed on. The diagram below provides a high-level overview of how external requests are routed via an internet-facing load balancer to the correct service via Ingress. Traffic flow (diagram) Inbound client request DNS routes request to appropriate LB LB forwards request to internal Ingress Ingress controller performs traffic routing lookup via Ingress object(s) Ingress forwards request to appropriate service based on Ingress object routing rule Service forwards request to appropriate pod Pod handles request Request body size By default the maximum allowed size for the request body is set to 250MB . If the size in a request exceeds the maximum size of the client request body, an 413 error will be returned to the client. The maximum request body can be configured by changing the value of maxBodySize in values.yaml . Volumes \u00b6 The Data Center products make use of filesystem storage. Each DC node has its own local-home volume, and all nodes in the DC cluster share a single shared-home volume. By default, the Helm charts will configure all of these volumes as ephemeral emptyDir volumes. This makes it possible to install the charts without configuring any volume management, but comes with two big caveats: Any data stored in the local-home or shared-home will be lost every time a pod starts. Whilst the data that is stored in local-home can generally be regenerated (e.g. from the database), this can be a very expensive process that sometimes requires manual intervention. For these reasons, the default volume configuration of the Helm charts is suitable only for running a single DC pod for evaluation purposes. Proper volume management needs to be configured in order for the data to survive restarts, and for multi-pod DC clusters to operate correctly. While you are free to configure your Kubernetes volume management in any way you wish, within the constraints imposed by the products, the recommended setup is to use Kubernetes PersistentVolumes and PersistentVolumeClaims . The local-home volume requires a PersistentVolume with ReadWriteOnce (RWO) capability, and shared-home requires a PersistentVolume with ReadWriteMany (RWX) capability. Typically, this will be an NFS volume provided as part of your infrastructure, but some public-cloud Kubernetes engines provide their own RWX volumes (e.g. AWS EFS and Azure Files ). While this entails a higher upfront setup effort, it gives the best flexibility. Volumes configuration \u00b6 By default, the charts will configure the local-home and shared-home values as follows: volumes : - name : local-home emptyDir : {} - name : shared-home emptyDir : {} As explained above, this default configuration is suitable only for evaluation or testing purposes. Proper volume management needs to be configured. In order to enable the persistence of data stored in these volumes, it is necessary to replace these volumes with something else. The recommended way is to enable the use of PersistentVolume and PersistentVolumeClaim for both volumes, using your install-specific values.yaml file, for example: volumes : localHome : persistentVolumeClaim : create : true shared-home : persistentVolumeClaim : create : true This will result in each pod in the StatefulSet creating a local-home PersistentVolumeClaim of type ReadWriteOnce , and a single PersistentVolumeClaim of type ReadWriteMany being created for the shared-home . For each PersistentVolumeClaim created by the chart, a suitable PersistentVolume needs to be made available prior to installation. These can be provisioned either statically or dynamically, using an auto-provisioner. An alternative to PersistentVolumeClaims is to use inline volume definitions, either for local-home or shared-home (or both), for example: volumes : localHome : customVolume : hostPath : path : /path/to/my/data shared-home : customVolume : nfs : server : mynfsserver path : /export/path Generally, any valid Kubernetes volume resource definition can be substituted here. However, as mentioned previously, externalising the volume definitions using PersistentVolumes is the strongly recommended approach. Volumes examples \u00b6 Bitbucket needs a dedicated NFS server providing persistence for a shared home. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but you can use this example as a guide: Implementation of an NFS Server for Bitbucket . We have an example detailing how an existing EFS filesystem can be created and consumed using static provisioning: Shared storage - utilizing AWS EFS-backed filesystem . You can also refer to an example on how a Kubernetes cluster and helm deployment can be configured to utilize AWS EBS backed volumes: Local storage - utilizing AWS EBS-backed volumes . Additional volumes \u00b6 In addition to the local-home and shared-home volumes that are always attached to the product pods, you can attach your own volumes for your own purposes, and mount them into the product container. Use the additional (under volumes ) and additionalVolumeMounts values to both attach the volumes and mount them in to the product container. This might be useful if, for example, you have a custom plugin that requires its own filesystem storage. Example: jira : additionalVolumeMounts : - volumeName : my-volume mountPath : /path/to/mount volumes : additional : - name : my-volume persistentVolumeClaim : claimName : my-volume-claim Database connectivity \u00b6 The products need to be supplied with the information they need to connect to the database service. Configuration for each product is mostly the same, with some small differences. database.url \u00b6 All products require the JDBC URL of the database. The format if this URL depends on the JDBC driver being used, but some examples are: Vendor JDBC driver class Example JDBC URL PostgreSQL org.postgresql.Driver jdbc:postgresql://<dbhost>:5432/<dbname> MySQL com.mysql.jdbc.Driver jdbc:mysql://<dbhost>/<dbname> SQL Server com.microsoft.sqlserver.jdbc.SQLServerDriver jdbc:sqlserver://<dbhost>:1433;databaseName=<dbname> Oracle oracle.jdbc.OracleDriver jdbc:oracle:thin:@<dbhost>:1521:<SID> Database creation The Atlassian product doesn't automatically create the database, <dbname> , in the JDBC URL , so you need to manually create a user and database for the used database instance. Details on how to create product-specific databases can be found below: Jira Connect Jira to an external database Confluence Connect Confluence to an external database Bitbucket Connect Bitbucket to an external database Crowd Connect Crowd to an external database database.driver \u00b6 Jira and Bitbucket require the JDBC driver class to be specified (Confluence will autoselect this based on the database.type value, see below). The JDBC driver must correspond to the JDBC URL used; see the table above for example driver classes. Note that the products only ship with certain JDBC drivers installed, depending on the license conditions of those drivers. Non-bundled DB drivers MySQL and Oracle database drivers are not shipped with the products due to licensing restrictions. You will need to provide additionalLibraries configuration . database.type \u00b6 Jira and Confluence both require this value to be specified, this declares the database engine to be used. The acceptable values for this include: Vendor Jira Confluence PostgreSQL postgres72 postgresql MySQL mysql57 / mysql8 mysql SQL Server mssql mssql Oracle oracle10g oracle database.credentials \u00b6 All products can have their database connectivity and credentials specified either interactively during first-time setup, or automatically by specifying certain configuration via Kubernetes. Depending on the product, the database.type , database.url and database.driver chart values can be provided. In addition, the database username and password can be provided via a Kubernetes secret , with the secret name specified with the database.credentials.secretName chart value. When all the required information is provided in this way, the database connectivity configuration screen will be bypassed during product setup. Namespace \u00b6 The Helm charts are not opinionated whether they have a Kubernetes namespace to themselves. If you wish, you can run multiple Helm releases of the same product in the same namespace. Clustering \u00b6 By default, the Helm charts will not configure the products for Data Center clustering. In order to enable clustering, the enabled property for clustering must be set to true . Jira jira : clustering : enabled : true Confluence confluence : clustering : enabled : true Bitbucket bitbucket : clustering : enabled : true In addition, the shared-home volume must be correctly configured as a ReadWriteMany (RWX) filesystem (e.g. NFS, AWS EFS and Azure Files ) Additional libraries & plugins \u00b6 The products' Docker images contain the default set of bundled libraries and plugins. Additional libraries and plugins can be mounted into the product containers during the Helm install. One such use case for this is mounting JDBC drivers that are not shipped with the products' by default. To make use of this mechanism, the additional files need to be available as part of a Kubernetes volume. Options here include putting them into the shared-home volume that's required as part of the prerequisites . Alternatively, you can create a custom PersistenVolume for them, as long as it has ReadOnlyMany capability. Custom volumes for loading libraries If you're not using the shared-home volume, then you can declare your own custom volume, by following the Additional volumes section above. You could even store the files as a ConfigMap that gets mounted as a volume, but you're likely to run into file size limitations there. Assuming that the existing shared-home volume is used for this, then the only configuration required is to specify the additionalLibraries in your values.yaml file, e.g. jira : additionalLibraries : - volumeName : shared-home subDirectory : mylibs fileName : lib1.jar - volumeName : shared-home subDirectory : mylibs fileName : lib2.jar This will mount the lib1.jar and lib2.jar from the mylibs sub-directory from shared-home into the appropriate place in the container. Similarly, you can use additionalBundledPlugins to load product plugins into the container. System plugin Plugins installed via this method will appear as system plugins rather than user plugins. An alternative to this method is to install the plugins via \"Manage Apps\" in the product system administration UI. For more details on the above, and how 3rd party libraries can be supplied to a Pod see the example External libraries and plugins CPU and memory requests \u00b6 The Helm charts allow you to specify container-level CPU and memory resource requests and limits e.g. jira : resources : container : requests : cpu : \"4\" memory : \"8G\" By default, the Helm Charts have no container-level resource limits, however there are default requests that are set. Specifying these values is fine for CPU limits/requests, but for memory resources it is also necessary to configure the JVM's memory limits. By default, the JVM maximum heap size is set to 1 GB, so if you increase (or decrease) the container memory resources as above, you also need to change the JVM's max heap size, otherwise the JVM won't take advantage of the extra available memory (or it'll crash if there isn't enough). You specify the JVM memory limits like this: jira : resources : jvm : maxHeap : \"8g\" Another difficulty for specifying memory resources is that the JVM requires additional overheads over and above the max heap size, and the container resources need to take account of that. A safe rule-of-thumb would be for the container to request 2x the value of the max heap for the JVM. This requirement to configure both the container memory and JVM heap will hopefully be removed. You can read more about resource scaling and resource requests and limits . Additional containers \u00b6 The Helm charts allow you to add your own container and initContainer entries to the product pods. Use the additionalContainers and additionalInitContainers stanzas within the values.yaml for this. One use-case for an additional container would be to attach a sidecar container to the product pods. Additional options \u00b6 The Helm charts also allow you to specify: additionalLabels tolerations , nodeSelectors affinities . These are standard Kubernetes structures that will be included in the pods.","title":"Configuration"},{"location":"userguide/CONFIGURATION/#configuration","text":"","title":"Configuration"},{"location":"userguide/CONFIGURATION/#ingress","text":"In order to make the Atlassian product available from outside of the Kubernetes cluster, a suitable HTTP/HTTPS ingress controller needs to be installed. The standard Kubernetes Ingress resource is not flexible enough for our needs, so a third-party ingress controller and resource definition must be provided. The exact details of the Ingress will be highly site-specific. These Helm charts were tested using the NGINX Ingress Controller . We also provide example instructions on how this controller can be installed and configured. The charts themselves provide a template for Ingress resource rules to be utilised by the provisioned controller. These include all required annotations and optional TLS configuration for the NGINX Ingress Controller. Some key considerations to note when configuring the controller are: Ingress At a minimum, the ingress needs the ability to support long request timeouts, as well as session affinity (aka \"sticky sessions\"). The Ingress Resource provided as part of the Helm charts is geared toward the NGINX Ingress Controller and can be configured via the ingress stanza in the appropriate values.yaml . Some key aspects that can be configured include: Usage of the NGINX Ingress Controller Ingress Controller annotations The request max body size The hostname of the ingress resource When installed, with the provided configuration , the NGINX Ingress Controller will provision an internet-facing (see diagram below) load balancer on your behalf. The load balancer should either support the Proxy Protocol or allow for the forwarding of X-Forwarded-* headers. This ensures any backend redirects are done so over the correct protocol. If the X-Forwarded-* headers are being used, then enable the use-forwarded-headers option on the controllers ConfigMap . This ensures that these headers are appropriately passed on. The diagram below provides a high-level overview of how external requests are routed via an internet-facing load balancer to the correct service via Ingress. Traffic flow (diagram) Inbound client request DNS routes request to appropriate LB LB forwards request to internal Ingress Ingress controller performs traffic routing lookup via Ingress object(s) Ingress forwards request to appropriate service based on Ingress object routing rule Service forwards request to appropriate pod Pod handles request Request body size By default the maximum allowed size for the request body is set to 250MB . If the size in a request exceeds the maximum size of the client request body, an 413 error will be returned to the client. The maximum request body can be configured by changing the value of maxBodySize in values.yaml .","title":" Ingress"},{"location":"userguide/CONFIGURATION/#volumes","text":"The Data Center products make use of filesystem storage. Each DC node has its own local-home volume, and all nodes in the DC cluster share a single shared-home volume. By default, the Helm charts will configure all of these volumes as ephemeral emptyDir volumes. This makes it possible to install the charts without configuring any volume management, but comes with two big caveats: Any data stored in the local-home or shared-home will be lost every time a pod starts. Whilst the data that is stored in local-home can generally be regenerated (e.g. from the database), this can be a very expensive process that sometimes requires manual intervention. For these reasons, the default volume configuration of the Helm charts is suitable only for running a single DC pod for evaluation purposes. Proper volume management needs to be configured in order for the data to survive restarts, and for multi-pod DC clusters to operate correctly. While you are free to configure your Kubernetes volume management in any way you wish, within the constraints imposed by the products, the recommended setup is to use Kubernetes PersistentVolumes and PersistentVolumeClaims . The local-home volume requires a PersistentVolume with ReadWriteOnce (RWO) capability, and shared-home requires a PersistentVolume with ReadWriteMany (RWX) capability. Typically, this will be an NFS volume provided as part of your infrastructure, but some public-cloud Kubernetes engines provide their own RWX volumes (e.g. AWS EFS and Azure Files ). While this entails a higher upfront setup effort, it gives the best flexibility.","title":" Volumes"},{"location":"userguide/CONFIGURATION/#volumes-configuration","text":"By default, the charts will configure the local-home and shared-home values as follows: volumes : - name : local-home emptyDir : {} - name : shared-home emptyDir : {} As explained above, this default configuration is suitable only for evaluation or testing purposes. Proper volume management needs to be configured. In order to enable the persistence of data stored in these volumes, it is necessary to replace these volumes with something else. The recommended way is to enable the use of PersistentVolume and PersistentVolumeClaim for both volumes, using your install-specific values.yaml file, for example: volumes : localHome : persistentVolumeClaim : create : true shared-home : persistentVolumeClaim : create : true This will result in each pod in the StatefulSet creating a local-home PersistentVolumeClaim of type ReadWriteOnce , and a single PersistentVolumeClaim of type ReadWriteMany being created for the shared-home . For each PersistentVolumeClaim created by the chart, a suitable PersistentVolume needs to be made available prior to installation. These can be provisioned either statically or dynamically, using an auto-provisioner. An alternative to PersistentVolumeClaims is to use inline volume definitions, either for local-home or shared-home (or both), for example: volumes : localHome : customVolume : hostPath : path : /path/to/my/data shared-home : customVolume : nfs : server : mynfsserver path : /export/path Generally, any valid Kubernetes volume resource definition can be substituted here. However, as mentioned previously, externalising the volume definitions using PersistentVolumes is the strongly recommended approach.","title":"Volumes configuration"},{"location":"userguide/CONFIGURATION/#volumes-examples","text":"Bitbucket needs a dedicated NFS server providing persistence for a shared home. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but you can use this example as a guide: Implementation of an NFS Server for Bitbucket . We have an example detailing how an existing EFS filesystem can be created and consumed using static provisioning: Shared storage - utilizing AWS EFS-backed filesystem . You can also refer to an example on how a Kubernetes cluster and helm deployment can be configured to utilize AWS EBS backed volumes: Local storage - utilizing AWS EBS-backed volumes .","title":"Volumes examples"},{"location":"userguide/CONFIGURATION/#additional-volumes","text":"In addition to the local-home and shared-home volumes that are always attached to the product pods, you can attach your own volumes for your own purposes, and mount them into the product container. Use the additional (under volumes ) and additionalVolumeMounts values to both attach the volumes and mount them in to the product container. This might be useful if, for example, you have a custom plugin that requires its own filesystem storage. Example: jira : additionalVolumeMounts : - volumeName : my-volume mountPath : /path/to/mount volumes : additional : - name : my-volume persistentVolumeClaim : claimName : my-volume-claim","title":"Additional volumes"},{"location":"userguide/CONFIGURATION/#database-connectivity","text":"The products need to be supplied with the information they need to connect to the database service. Configuration for each product is mostly the same, with some small differences.","title":" Database connectivity"},{"location":"userguide/CONFIGURATION/#databaseurl","text":"All products require the JDBC URL of the database. The format if this URL depends on the JDBC driver being used, but some examples are: Vendor JDBC driver class Example JDBC URL PostgreSQL org.postgresql.Driver jdbc:postgresql://<dbhost>:5432/<dbname> MySQL com.mysql.jdbc.Driver jdbc:mysql://<dbhost>/<dbname> SQL Server com.microsoft.sqlserver.jdbc.SQLServerDriver jdbc:sqlserver://<dbhost>:1433;databaseName=<dbname> Oracle oracle.jdbc.OracleDriver jdbc:oracle:thin:@<dbhost>:1521:<SID> Database creation The Atlassian product doesn't automatically create the database, <dbname> , in the JDBC URL , so you need to manually create a user and database for the used database instance. Details on how to create product-specific databases can be found below: Jira Connect Jira to an external database Confluence Connect Confluence to an external database Bitbucket Connect Bitbucket to an external database Crowd Connect Crowd to an external database","title":"database.url"},{"location":"userguide/CONFIGURATION/#databasedriver","text":"Jira and Bitbucket require the JDBC driver class to be specified (Confluence will autoselect this based on the database.type value, see below). The JDBC driver must correspond to the JDBC URL used; see the table above for example driver classes. Note that the products only ship with certain JDBC drivers installed, depending on the license conditions of those drivers. Non-bundled DB drivers MySQL and Oracle database drivers are not shipped with the products due to licensing restrictions. You will need to provide additionalLibraries configuration .","title":"database.driver"},{"location":"userguide/CONFIGURATION/#databasetype","text":"Jira and Confluence both require this value to be specified, this declares the database engine to be used. The acceptable values for this include: Vendor Jira Confluence PostgreSQL postgres72 postgresql MySQL mysql57 / mysql8 mysql SQL Server mssql mssql Oracle oracle10g oracle","title":"database.type"},{"location":"userguide/CONFIGURATION/#databasecredentials","text":"All products can have their database connectivity and credentials specified either interactively during first-time setup, or automatically by specifying certain configuration via Kubernetes. Depending on the product, the database.type , database.url and database.driver chart values can be provided. In addition, the database username and password can be provided via a Kubernetes secret , with the secret name specified with the database.credentials.secretName chart value. When all the required information is provided in this way, the database connectivity configuration screen will be bypassed during product setup.","title":"database.credentials"},{"location":"userguide/CONFIGURATION/#namespace","text":"The Helm charts are not opinionated whether they have a Kubernetes namespace to themselves. If you wish, you can run multiple Helm releases of the same product in the same namespace.","title":" Namespace"},{"location":"userguide/CONFIGURATION/#clustering","text":"By default, the Helm charts will not configure the products for Data Center clustering. In order to enable clustering, the enabled property for clustering must be set to true . Jira jira : clustering : enabled : true Confluence confluence : clustering : enabled : true Bitbucket bitbucket : clustering : enabled : true In addition, the shared-home volume must be correctly configured as a ReadWriteMany (RWX) filesystem (e.g. NFS, AWS EFS and Azure Files )","title":" Clustering"},{"location":"userguide/CONFIGURATION/#additional-libraries-plugins","text":"The products' Docker images contain the default set of bundled libraries and plugins. Additional libraries and plugins can be mounted into the product containers during the Helm install. One such use case for this is mounting JDBC drivers that are not shipped with the products' by default. To make use of this mechanism, the additional files need to be available as part of a Kubernetes volume. Options here include putting them into the shared-home volume that's required as part of the prerequisites . Alternatively, you can create a custom PersistenVolume for them, as long as it has ReadOnlyMany capability. Custom volumes for loading libraries If you're not using the shared-home volume, then you can declare your own custom volume, by following the Additional volumes section above. You could even store the files as a ConfigMap that gets mounted as a volume, but you're likely to run into file size limitations there. Assuming that the existing shared-home volume is used for this, then the only configuration required is to specify the additionalLibraries in your values.yaml file, e.g. jira : additionalLibraries : - volumeName : shared-home subDirectory : mylibs fileName : lib1.jar - volumeName : shared-home subDirectory : mylibs fileName : lib2.jar This will mount the lib1.jar and lib2.jar from the mylibs sub-directory from shared-home into the appropriate place in the container. Similarly, you can use additionalBundledPlugins to load product plugins into the container. System plugin Plugins installed via this method will appear as system plugins rather than user plugins. An alternative to this method is to install the plugins via \"Manage Apps\" in the product system administration UI. For more details on the above, and how 3rd party libraries can be supplied to a Pod see the example External libraries and plugins","title":" Additional libraries &amp; plugins"},{"location":"userguide/CONFIGURATION/#cpu-and-memory-requests","text":"The Helm charts allow you to specify container-level CPU and memory resource requests and limits e.g. jira : resources : container : requests : cpu : \"4\" memory : \"8G\" By default, the Helm Charts have no container-level resource limits, however there are default requests that are set. Specifying these values is fine for CPU limits/requests, but for memory resources it is also necessary to configure the JVM's memory limits. By default, the JVM maximum heap size is set to 1 GB, so if you increase (or decrease) the container memory resources as above, you also need to change the JVM's max heap size, otherwise the JVM won't take advantage of the extra available memory (or it'll crash if there isn't enough). You specify the JVM memory limits like this: jira : resources : jvm : maxHeap : \"8g\" Another difficulty for specifying memory resources is that the JVM requires additional overheads over and above the max heap size, and the container resources need to take account of that. A safe rule-of-thumb would be for the container to request 2x the value of the max heap for the JVM. This requirement to configure both the container memory and JVM heap will hopefully be removed. You can read more about resource scaling and resource requests and limits .","title":" CPU and memory requests"},{"location":"userguide/CONFIGURATION/#additional-containers","text":"The Helm charts allow you to add your own container and initContainer entries to the product pods. Use the additionalContainers and additionalInitContainers stanzas within the values.yaml for this. One use-case for an additional container would be to attach a sidecar container to the product pods.","title":" Additional containers"},{"location":"userguide/CONFIGURATION/#additional-options","text":"The Helm charts also allow you to specify: additionalLabels tolerations , nodeSelectors affinities . These are standard Kubernetes structures that will be included in the pods.","title":" Additional options"},{"location":"userguide/INSTALLATION/","text":"Installation \u00b6 Follow these instructions to install your Atlassian product using the Helm charts. Before you proceed with the installation, make sure you have followed the Prerequisites guide . 1. Add the Helm chart repository \u00b6 Add the Helm chart repository to your local Helm installation: helm repo add atlassian-data-center \\ https://atlassian.github.io/data-center-helm-charts Update the repository: helm repo update 2. Obtain values.yaml \u00b6 Obtain the default product values.yaml file from the chart: helm show values atlassian-data-center/<product> > values.yaml 3. Configure database \u00b6 Using the values.yaml file obtained in step 2 , configure the usage of the database provisioned as part of the prerequisites . Automated setup steps By providing all the required database values, you will bypass the database connectivity configuration during the product setup. Migration If you are migrating an existing Data Center product to Kubernetes, use the values of your product's database. See Migration guide . Create a Kubernetes secret to store the connectivity details of the database: kubectl create secret generic <secret_name> --from-literal = username = '<db_username>' --from-literal = password = '<db_password>' Using the Kubernetes secret, update the database stanza within values.yaml appropriately. Refer to the commentary within the values.yaml file for additional details on how to configure the remaining database values: database : type : <db_type> url : <jdbc_url> driver : <engine_driver> credentials : secretName : <secret_name> usernameSecretKey : username passwordSecretKey : password Database connectivity For additional information on how the above values should be configured, see the Database connectivity section of the configuration guide . Read about Kubernetes secrets . 4. Configure Ingress \u00b6 Using the values.yaml file obtained in step 2 , configure the Ingress controller provisioned as part of the Prerequisites . The values you provide here will be used to provision an Ingress resource for the controller. Refer to the associated comments within the values.yaml file for additional details on how to configure the Ingress resource: ingress : create : true #1. Setting true here will create an Ingress resource nginx : true #2. If using the ingress-nginx controller set this property to true maxBodySize : 250m host : <dns_host_name> #2. Hosts can be precise matches (for example \u201cfoo.bar.com\u201d) or a wildcard (for example \u201c*.foo.com\u201d). path : \"/\" annotations : cert-manager.io/issuer : <certificate_issuer> https : true tlsSecretName : <tls_certificate_name> Ingress configuration For additional details on Ingress controllers see the Ingress section of the configuration guide . See an example of how to set up a controller . 5. Configure persistent storage \u00b6 Using the values.yaml file obtained in step 2 , configure the shared-home that was provisioned as part of the Prerequisites . See shared home example . If you are migrating an existing Data Center product to Kubernetes , use the values of your product's shared home. volumes : sharedHome : customVolume : persistentVolumeClaim : claimName : <pvc_name> Each pod will also require its own local-home storage. This can be configured with a StorageClass , as can be seen in the local home example . Having created the StorageClass , update values.yaml to make use of it: volumes : localHome : persistentVolumeClaim : create : true storageClassName : <storage-class-name> Volume configuration For more details, refer to the Volumes section of the configuration guide . Bitbucket shared storage Bitbucket needs a dedicated NFS server providing persistence for a shared home. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but you can use this example as a guide: Implementation of an NFS Server for Bitbucket . 6. Configure clustering \u00b6 By default, the Helm charts will not configure the products for Data Center clustering. You can enable clustering in the values.yaml file: clustering : enabled : true 7. Configure license \u00b6 You can configure the product license if you provide a license stanzas within the values.yaml obtained in step 2 . To do that, create a Kubernetes secret to hold the product license: kubectl create secret generic <license_secret_name> --from-literal = license-key = '<product_license_key>' Update the values.yaml file with the secrets: license : secretName : <secret_name> secretKey : license-key Sysadmin credentials for Bitbucket Bitbucket is slightly different from the other products in that it can be completely configured during deployment, meaning no manual setup is required. To do this, you need to update the sysadminCredentials and also provide the license stanza from the previous step. Create a Kubernetes secret to hold the Bitbucket system administrator credentials: kubectl create secret generic <sysadmin_creds_secret_name> --from-literal = username = '<sysadmin_username>' --from-literal = password = '<sysadmin_password>' --from-literal = displayName = '<sysadmin_display_name>' --from-literal = emailAddress = '<sysadmin_email>' Update the values.yaml file with the secrets: sysadminCredentials : secretName : <sysadmin_creds_secret_name> usernameSecretKey : username passwordSecretKey : password displayNameSecretKey : displayName emailAddressSecretKey : emailAddress 8. Install your chosen product \u00b6 helm install <release-name> \\ atlassian-data-center/<product> \\ --namespace <namespace> \\ --version <chart-version> \\ --values values.yaml Values & flags <release-name> the name of your deployment. You can also use --generate-name . <product> the product to install. Options include jira , confluence , bitbucket , or crowd . <namespace> optional flag for categorizing installed resources. <chart-version> optional flag for defining the chart version to be used. If omitted, the latest version of the chart will be used. values.yaml optional flag for defining your site-specific configuration information. If omitted, the chart config default will be used. Add --wait if you wish the installation command to block until all of the deployed Kubernetes resources are ready, but be aware that this may wait for several minutes if anything is mis-configured. Elasticsearch for Bitbucket We highly recommend you use an external Elasticsearch installation for Bitbucket. When you run more than one node you need to have a separate Elasticsearch cluster to enable code search. See Bitbucket Elasticsearch recommendations . 9. Test your deployed product \u00b6 Make sure the service pod/s are running, then test your deployed product: helm test <release-name> --logs --namespace <namespace> This will run some basic smoke tests against the deployed release. If any of these tests fail, it is likely that the deployment was not successful. Check the status of the deployed resources for any obvious errors that may have caused the failure. 10. Complete product setup \u00b6 Using the service URL provided by Helm post install, open your product in a web browser and complete the setup via the setup wizard. Uninstall \u00b6 The deployment and all of its associated resources can be un-installed with the following command: helm uninstall <release-name> atlassian-data-center/<product>","title":"Installation"},{"location":"userguide/INSTALLATION/#installation","text":"Follow these instructions to install your Atlassian product using the Helm charts. Before you proceed with the installation, make sure you have followed the Prerequisites guide .","title":"Installation"},{"location":"userguide/INSTALLATION/#1-add-the-helm-chart-repository","text":"Add the Helm chart repository to your local Helm installation: helm repo add atlassian-data-center \\ https://atlassian.github.io/data-center-helm-charts Update the repository: helm repo update","title":"1. Add the Helm chart repository"},{"location":"userguide/INSTALLATION/#2-obtain-valuesyaml","text":"Obtain the default product values.yaml file from the chart: helm show values atlassian-data-center/<product> > values.yaml","title":"2. Obtain values.yaml"},{"location":"userguide/INSTALLATION/#3-configure-database","text":"Using the values.yaml file obtained in step 2 , configure the usage of the database provisioned as part of the prerequisites . Automated setup steps By providing all the required database values, you will bypass the database connectivity configuration during the product setup. Migration If you are migrating an existing Data Center product to Kubernetes, use the values of your product's database. See Migration guide . Create a Kubernetes secret to store the connectivity details of the database: kubectl create secret generic <secret_name> --from-literal = username = '<db_username>' --from-literal = password = '<db_password>' Using the Kubernetes secret, update the database stanza within values.yaml appropriately. Refer to the commentary within the values.yaml file for additional details on how to configure the remaining database values: database : type : <db_type> url : <jdbc_url> driver : <engine_driver> credentials : secretName : <secret_name> usernameSecretKey : username passwordSecretKey : password Database connectivity For additional information on how the above values should be configured, see the Database connectivity section of the configuration guide . Read about Kubernetes secrets .","title":"3. Configure database"},{"location":"userguide/INSTALLATION/#4-configure-ingress","text":"Using the values.yaml file obtained in step 2 , configure the Ingress controller provisioned as part of the Prerequisites . The values you provide here will be used to provision an Ingress resource for the controller. Refer to the associated comments within the values.yaml file for additional details on how to configure the Ingress resource: ingress : create : true #1. Setting true here will create an Ingress resource nginx : true #2. If using the ingress-nginx controller set this property to true maxBodySize : 250m host : <dns_host_name> #2. Hosts can be precise matches (for example \u201cfoo.bar.com\u201d) or a wildcard (for example \u201c*.foo.com\u201d). path : \"/\" annotations : cert-manager.io/issuer : <certificate_issuer> https : true tlsSecretName : <tls_certificate_name> Ingress configuration For additional details on Ingress controllers see the Ingress section of the configuration guide . See an example of how to set up a controller .","title":"4. Configure Ingress"},{"location":"userguide/INSTALLATION/#5-configure-persistent-storage","text":"Using the values.yaml file obtained in step 2 , configure the shared-home that was provisioned as part of the Prerequisites . See shared home example . If you are migrating an existing Data Center product to Kubernetes , use the values of your product's shared home. volumes : sharedHome : customVolume : persistentVolumeClaim : claimName : <pvc_name> Each pod will also require its own local-home storage. This can be configured with a StorageClass , as can be seen in the local home example . Having created the StorageClass , update values.yaml to make use of it: volumes : localHome : persistentVolumeClaim : create : true storageClassName : <storage-class-name> Volume configuration For more details, refer to the Volumes section of the configuration guide . Bitbucket shared storage Bitbucket needs a dedicated NFS server providing persistence for a shared home. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but you can use this example as a guide: Implementation of an NFS Server for Bitbucket .","title":"5. Configure persistent storage"},{"location":"userguide/INSTALLATION/#6-configure-clustering","text":"By default, the Helm charts will not configure the products for Data Center clustering. You can enable clustering in the values.yaml file: clustering : enabled : true","title":"6. Configure clustering"},{"location":"userguide/INSTALLATION/#7-configure-license","text":"You can configure the product license if you provide a license stanzas within the values.yaml obtained in step 2 . To do that, create a Kubernetes secret to hold the product license: kubectl create secret generic <license_secret_name> --from-literal = license-key = '<product_license_key>' Update the values.yaml file with the secrets: license : secretName : <secret_name> secretKey : license-key Sysadmin credentials for Bitbucket Bitbucket is slightly different from the other products in that it can be completely configured during deployment, meaning no manual setup is required. To do this, you need to update the sysadminCredentials and also provide the license stanza from the previous step. Create a Kubernetes secret to hold the Bitbucket system administrator credentials: kubectl create secret generic <sysadmin_creds_secret_name> --from-literal = username = '<sysadmin_username>' --from-literal = password = '<sysadmin_password>' --from-literal = displayName = '<sysadmin_display_name>' --from-literal = emailAddress = '<sysadmin_email>' Update the values.yaml file with the secrets: sysadminCredentials : secretName : <sysadmin_creds_secret_name> usernameSecretKey : username passwordSecretKey : password displayNameSecretKey : displayName emailAddressSecretKey : emailAddress","title":"7. Configure license"},{"location":"userguide/INSTALLATION/#8-install-your-chosen-product","text":"helm install <release-name> \\ atlassian-data-center/<product> \\ --namespace <namespace> \\ --version <chart-version> \\ --values values.yaml Values & flags <release-name> the name of your deployment. You can also use --generate-name . <product> the product to install. Options include jira , confluence , bitbucket , or crowd . <namespace> optional flag for categorizing installed resources. <chart-version> optional flag for defining the chart version to be used. If omitted, the latest version of the chart will be used. values.yaml optional flag for defining your site-specific configuration information. If omitted, the chart config default will be used. Add --wait if you wish the installation command to block until all of the deployed Kubernetes resources are ready, but be aware that this may wait for several minutes if anything is mis-configured. Elasticsearch for Bitbucket We highly recommend you use an external Elasticsearch installation for Bitbucket. When you run more than one node you need to have a separate Elasticsearch cluster to enable code search. See Bitbucket Elasticsearch recommendations .","title":"8. Install your chosen product"},{"location":"userguide/INSTALLATION/#9-test-your-deployed-product","text":"Make sure the service pod/s are running, then test your deployed product: helm test <release-name> --logs --namespace <namespace> This will run some basic smoke tests against the deployed release. If any of these tests fail, it is likely that the deployment was not successful. Check the status of the deployed resources for any obvious errors that may have caused the failure.","title":"9. Test your deployed product"},{"location":"userguide/INSTALLATION/#10-complete-product-setup","text":"Using the service URL provided by Helm post install, open your product in a web browser and complete the setup via the setup wizard.","title":"10. Complete product setup"},{"location":"userguide/INSTALLATION/#uninstall","text":"The deployment and all of its associated resources can be un-installed with the following command: helm uninstall <release-name> atlassian-data-center/<product>","title":"Uninstall"},{"location":"userguide/MIGRATION/","text":"Migration \u00b6 If you already have an existing Data Center product deployment, you can migrate it to a Kubernetes cluster using the Data Center Helm charts. You will need to migrate your database and your shared home, then all you need to do is to follow the Installation guide , using your migrated resources instead of provisioning new ones. Migrating your database \u00b6 To migrate your database, you should point the Helm charts to the existing database or to a migrated version of the database. Do this by updating the database stanza in the values.yaml file as explained in the Configure database step in the installation guide . Migrating your shared home \u00b6 Application nodes should have access to a shared directory in the same path. Examples of what the shared file system stores include plugins, shared caches, repositories, attachments, and avatars. Configure your shared home by updating the sharedHome stanza in the values.yaml file as explained in the Configure persistent storage step in the installation guide . Helpful links \u00b6 Atlassian Data Center migration plan - gives some guidance on overall process, organizational preparedness, estimated time frames, and app compatibility. Atlassian Data Center migration checklist - also provides useful tests and checks to perform throughout the moving process. Migrating to another database - describes how to migrate your data from your existing database to another database: Migrating Confluence to another database Migrating Jira to another database Availability Zone proximity For better performance consider co-locating your migrated database in the same Availability Zone (AZ) as your product nodes. Database-heavy operations, such as full re-index, become significantly faster when the database is collocated with the Data Center node in the same AZ. However we don't recommend this if you're running critical workloads.","title":"Migration"},{"location":"userguide/MIGRATION/#migration","text":"If you already have an existing Data Center product deployment, you can migrate it to a Kubernetes cluster using the Data Center Helm charts. You will need to migrate your database and your shared home, then all you need to do is to follow the Installation guide , using your migrated resources instead of provisioning new ones.","title":"Migration"},{"location":"userguide/MIGRATION/#migrating-your-database","text":"To migrate your database, you should point the Helm charts to the existing database or to a migrated version of the database. Do this by updating the database stanza in the values.yaml file as explained in the Configure database step in the installation guide .","title":"Migrating your database"},{"location":"userguide/MIGRATION/#migrating-your-shared-home","text":"Application nodes should have access to a shared directory in the same path. Examples of what the shared file system stores include plugins, shared caches, repositories, attachments, and avatars. Configure your shared home by updating the sharedHome stanza in the values.yaml file as explained in the Configure persistent storage step in the installation guide .","title":"Migrating your shared home"},{"location":"userguide/MIGRATION/#helpful-links","text":"Atlassian Data Center migration plan - gives some guidance on overall process, organizational preparedness, estimated time frames, and app compatibility. Atlassian Data Center migration checklist - also provides useful tests and checks to perform throughout the moving process. Migrating to another database - describes how to migrate your data from your existing database to another database: Migrating Confluence to another database Migrating Jira to another database Availability Zone proximity For better performance consider co-locating your migrated database in the same Availability Zone (AZ) as your product nodes. Database-heavy operations, such as full re-index, become significantly faster when the database is collocated with the Data Center node in the same AZ. However we don't recommend this if you're running critical workloads.","title":"Helpful links"},{"location":"userguide/OPERATION/","text":"Operation \u00b6 Once you have installed your product , use this document if you want to scale your product, update your product, or see what examples we have. Managing resources \u00b6 You can scale your application by adding additonal pods or by managing available resources with requests and limits . Upgrading application \u00b6 Kubernetes update strategies \u00b6 Kubernetes provides two strategies to update applications managed by statefulset controllers: Rolling update \u00b6 The pods will be upgraded one by one until all pods run containers with the updated template. The upgrade is managed by Kubernetes and the user has limited control during the upgrade process, after having modified the template. This is the default upgrade strategy in Kubernetes. To perform a canary or multi-phase upgrade, a partition can be defined on the cluster and Kubernetes will upgrade just the nodes in that partition. The default implementation is based on RollingUpdate strategy with no partition defined. OnDelete strategy \u00b6 In this strategy users select the pod to upgrade by deleting it, and Kubernetes will replace it by creating a new pod based on the updated template. To select this strategy the following should be replaced with the current implementation of updateStrategy in the statefulset spec: updateStrategy : type : OnDelete Upgrade \u00b6 To learn about upgrading the Helm charts see Helm chart upgrade . To learn about upgrading the products without upgrading the Helm charts see Products upgrade . Examples \u00b6 Logging \u00b6 How to deploy an EFK stack to Kubernetes \u00b6 There are different methods to deploy an EFK stack. We provide two deployment methods, the first is deploying EFK locally on Kubernetes, and the second is using managed Elasticsearch outside the Kubernetes cluster. Please refer to Logging in Kubernetes .","title":"Operation"},{"location":"userguide/OPERATION/#operation","text":"Once you have installed your product , use this document if you want to scale your product, update your product, or see what examples we have.","title":"Operation"},{"location":"userguide/OPERATION/#managing-resources","text":"You can scale your application by adding additonal pods or by managing available resources with requests and limits .","title":"Managing resources"},{"location":"userguide/OPERATION/#upgrading-application","text":"","title":"Upgrading application"},{"location":"userguide/OPERATION/#kubernetes-update-strategies","text":"Kubernetes provides two strategies to update applications managed by statefulset controllers:","title":"Kubernetes update strategies"},{"location":"userguide/OPERATION/#rolling-update","text":"The pods will be upgraded one by one until all pods run containers with the updated template. The upgrade is managed by Kubernetes and the user has limited control during the upgrade process, after having modified the template. This is the default upgrade strategy in Kubernetes. To perform a canary or multi-phase upgrade, a partition can be defined on the cluster and Kubernetes will upgrade just the nodes in that partition. The default implementation is based on RollingUpdate strategy with no partition defined.","title":"Rolling update"},{"location":"userguide/OPERATION/#ondelete-strategy","text":"In this strategy users select the pod to upgrade by deleting it, and Kubernetes will replace it by creating a new pod based on the updated template. To select this strategy the following should be replaced with the current implementation of updateStrategy in the statefulset spec: updateStrategy : type : OnDelete","title":"OnDelete strategy"},{"location":"userguide/OPERATION/#upgrade","text":"To learn about upgrading the Helm charts see Helm chart upgrade . To learn about upgrading the products without upgrading the Helm charts see Products upgrade .","title":"Upgrade"},{"location":"userguide/OPERATION/#examples","text":"","title":"Examples"},{"location":"userguide/OPERATION/#logging","text":"","title":"Logging"},{"location":"userguide/OPERATION/#how-to-deploy-an-efk-stack-to-kubernetes","text":"There are different methods to deploy an EFK stack. We provide two deployment methods, the first is deploying EFK locally on Kubernetes, and the second is using managed Elasticsearch outside the Kubernetes cluster. Please refer to Logging in Kubernetes .","title":"How to deploy an EFK stack to Kubernetes"},{"location":"userguide/PREREQUISITES/","text":"Prerequisites \u00b6 Requirements \u00b6 In order to deploy Atlassian\u2019s Data Center products, the following is required: An understanding of Kubernetes and Helm concepts. kubectl v1.19 or later , must be compatible with your cluster. helm v3.3 or later. Environment setup \u00b6 Before installing the Data Center Helm charts you need to set up your environment: Create and connect to the Kubernetes cluster Provision an Ingress Controller Provision a database Configure a shared-home volume Configure a local-home volume Elasticsearch for Bitbucket We highly recommend you use an external Elasticsearch installation for Bitbucket. When you run more than one node you need to have a separate Elasticsearch cluster to enable code search. See Bitbucket Elasticsearch recommendations . Create and connect to the Kubernetes cluster \u00b6 In order to install the charts to your Kubernetes cluster (version 1.19+), your Kubernetes client config must be configured appropriately, and you must have the necessary permissions. It is up to you to set up security policies. See examples of provisioning Kubernetes clusters on cloud-based providers . Provision an Ingress Controller \u00b6 This step is necessary in order to make your Atlassian product available from outside of the Kubernetes cluster after deployment. The Kubernetes project supports and maintains ingress controllers for the major cloud providers including; AWS , GCE and nginx . There are also a number of open-source third-party projects available . Because different Kubernetes clusters use different ingress configurations/controllers, the Helm charts provide Ingress Object templates only. The Ingress resource provided as part of the Helm charts is geared toward the NGINX Ingress Controller and can be configured via the ingress stanza in the appropriate values.yaml (an alternative controller can be used). For more information about the Ingress controller go to the Ingress section of the configuration guide . See an example of provisioning an NGINX Ingress Controller . Provision a database \u00b6 Must be of a type and version supported by the Data Center product you wish to install: Jira Supported databases Confluence Supported databases Bitbucket Supported databases Crowd Supported databases Must be reachable from the product deployed within your Kubernetes cluster. The database service may be deployed within the same Kubernetes cluster as the Data Center product or elsewhere. The products need to be provided with the information they need to connect to the database service. Configuration for each product is mostly the same, with some small differences. For more information go to the Database connectivity section of the configuration guide . Reducing pod to database latency For better performance consider co-locating your database in the same Availability Zone (AZ) as your product nodes. Database-heavy operations, such as full re-index, become significantly faster when the database is collocated with the Data Center node in the same AZ. However we don't recommend this if you're running critical workloads. See an example of provisioning databases on cloud-based providers . Configure a shared-home volume \u00b6 All of the Data Center products require a shared network filesystem if they are to be operated in multi-node clusters. If no shared filesystem is available, the products can only be operated in single-node configuration. Some cloud based options for a shared filesystem include AWS EFS and Azure Files . You can also stand up your own NFS The logical representation of the chosen storage type within Kubernetes can be defined as PersistentVolumes with an associated PersistentVolumeClaims in a ReadWriteMany (RWX) access mode. For more information about volumes see the Volumes section of the configuration guide . See examples of creating shared storage . Configure local-home volume \u00b6 As with the shared-home , each pod requires its own volume for local-home . Each product needs this for defining operational data. If not defined, an emptyDir will be utilised. Although an emptyDir may be acceptable for evaluation purposes, we recommend that each pod is allocated its own volume. A local-home volume could be logically represented within the cluster using a StorageClass . This will dynamically provision an AWS EBS volume to each pod. An example of this strategy can be found the local storage example .","title":"Prerequisites"},{"location":"userguide/PREREQUISITES/#prerequisites","text":"","title":"Prerequisites"},{"location":"userguide/PREREQUISITES/#requirements","text":"In order to deploy Atlassian\u2019s Data Center products, the following is required: An understanding of Kubernetes and Helm concepts. kubectl v1.19 or later , must be compatible with your cluster. helm v3.3 or later.","title":"Requirements"},{"location":"userguide/PREREQUISITES/#environment-setup","text":"Before installing the Data Center Helm charts you need to set up your environment: Create and connect to the Kubernetes cluster Provision an Ingress Controller Provision a database Configure a shared-home volume Configure a local-home volume Elasticsearch for Bitbucket We highly recommend you use an external Elasticsearch installation for Bitbucket. When you run more than one node you need to have a separate Elasticsearch cluster to enable code search. See Bitbucket Elasticsearch recommendations .","title":"Environment setup"},{"location":"userguide/PREREQUISITES/#create-and-connect-to-the-kubernetes-cluster","text":"In order to install the charts to your Kubernetes cluster (version 1.19+), your Kubernetes client config must be configured appropriately, and you must have the necessary permissions. It is up to you to set up security policies. See examples of provisioning Kubernetes clusters on cloud-based providers .","title":" Create and connect to the Kubernetes cluster"},{"location":"userguide/PREREQUISITES/#provision-an-ingress-controller","text":"This step is necessary in order to make your Atlassian product available from outside of the Kubernetes cluster after deployment. The Kubernetes project supports and maintains ingress controllers for the major cloud providers including; AWS , GCE and nginx . There are also a number of open-source third-party projects available . Because different Kubernetes clusters use different ingress configurations/controllers, the Helm charts provide Ingress Object templates only. The Ingress resource provided as part of the Helm charts is geared toward the NGINX Ingress Controller and can be configured via the ingress stanza in the appropriate values.yaml (an alternative controller can be used). For more information about the Ingress controller go to the Ingress section of the configuration guide . See an example of provisioning an NGINX Ingress Controller .","title":" Provision an Ingress Controller"},{"location":"userguide/PREREQUISITES/#provision-a-database","text":"Must be of a type and version supported by the Data Center product you wish to install: Jira Supported databases Confluence Supported databases Bitbucket Supported databases Crowd Supported databases Must be reachable from the product deployed within your Kubernetes cluster. The database service may be deployed within the same Kubernetes cluster as the Data Center product or elsewhere. The products need to be provided with the information they need to connect to the database service. Configuration for each product is mostly the same, with some small differences. For more information go to the Database connectivity section of the configuration guide . Reducing pod to database latency For better performance consider co-locating your database in the same Availability Zone (AZ) as your product nodes. Database-heavy operations, such as full re-index, become significantly faster when the database is collocated with the Data Center node in the same AZ. However we don't recommend this if you're running critical workloads. See an example of provisioning databases on cloud-based providers .","title":" Provision a database"},{"location":"userguide/PREREQUISITES/#configure-a-shared-home-volume","text":"All of the Data Center products require a shared network filesystem if they are to be operated in multi-node clusters. If no shared filesystem is available, the products can only be operated in single-node configuration. Some cloud based options for a shared filesystem include AWS EFS and Azure Files . You can also stand up your own NFS The logical representation of the chosen storage type within Kubernetes can be defined as PersistentVolumes with an associated PersistentVolumeClaims in a ReadWriteMany (RWX) access mode. For more information about volumes see the Volumes section of the configuration guide . See examples of creating shared storage .","title":" Configure a shared-home volume"},{"location":"userguide/PREREQUISITES/#configure-local-home-volume","text":"As with the shared-home , each pod requires its own volume for local-home . Each product needs this for defining operational data. If not defined, an emptyDir will be utilised. Although an emptyDir may be acceptable for evaluation purposes, we recommend that each pod is allocated its own volume. A local-home volume could be logically represented within the cluster using a StorageClass . This will dynamically provision an AWS EBS volume to each pod. An example of this strategy can be found the local storage example .","title":" Configure local-home volume"},{"location":"userguide/resource_management/JIRA_INDEX_SNAPSHOT/","text":"Creating an initial index snapshot in Jira \u00b6 These steps should be followed to enable shared index snapshots with Jira: Log into the Jira instance as the Administrator Go to Settings -> System -> Indexing There should be no errors on this page i.e. If there are errors (as seen below) perform a Full re-index before proceeding Once the Full re-index is complete, scroll down to Index Recovery settings visible on the same page Take note of the current settings Temporarily change these values ( Edit Settings ), as depicted in the screenshot below. The cron expression will create an index snapshot every minute Wait for the snapshot to be created, by checking for an archive in <shared-home>/export/indexsnapshots When the snapshot is available, revert the settings noted in step 6 , or back to the defaults: Consider keeping the Enable index recovery setting so that it is set to ON Proceed with scaling the cluster as necessary","title":"Creating an initial index snapshot in Jira"},{"location":"userguide/resource_management/JIRA_INDEX_SNAPSHOT/#creating-an-initial-index-snapshot-in-jira","text":"These steps should be followed to enable shared index snapshots with Jira: Log into the Jira instance as the Administrator Go to Settings -> System -> Indexing There should be no errors on this page i.e. If there are errors (as seen below) perform a Full re-index before proceeding Once the Full re-index is complete, scroll down to Index Recovery settings visible on the same page Take note of the current settings Temporarily change these values ( Edit Settings ), as depicted in the screenshot below. The cron expression will create an index snapshot every minute Wait for the snapshot to be created, by checking for an archive in <shared-home>/export/indexsnapshots When the snapshot is available, revert the settings noted in step 6 , or back to the defaults: Consider keeping the Enable index recovery setting so that it is set to ON Proceed with scaling the cluster as necessary","title":"Creating an initial index snapshot in Jira"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/","text":"Resource requests and limits \u00b6 To ensure that Kubernetes appropriately schedules resources, the respective product values.yaml is configured with default cpu and memory resource request values . Resource requests \u00b6 The default resource requests that are used for each product are defined below. Note: these values are geared towards small data sets. For larger enterprise deployments refer to the Data Center infrastructure recommendations . Using the formula below, the memory specific values are derived from the default JVM requirements defined for each product's Docker container. Product CPU Memory Jira 2 2G Confluence 2 2G Bitbucket 2 2G Crowd 2 1G Synchrony container resources \u00b6 A Confluence cluster requires a Synchrony container. The default resource requests that are used for the Synchrony container are: cpu: 2 memory: 2.5G Memory request sizing \u00b6 Request sizing must allow for the size of the product JVM . That means the maximum heap size , minumum heap size and the reserved code cache size (if applicable) plus other JVM overheads, must be considered when defining the request memory size. As a rule of thumb the formula below can be used to deduce the appropriate request memory size. ( maxHeap + codeCache ) * 1 .5 Resource limits \u00b6 Environmental and hardware constraints are different for each deployment, therefore the product's values.yaml does not provide a resource limit definition. Resource usage limits can be defined by updating the commented out the resources.container.limits stanza within the appropriate values.yaml . For example: container : limits : cpu : \"2\" memory : \"4G\" requests : cpu : \"2\" memory : \"2G\"","title":"Resource requests and limits"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#resource-requests-and-limits","text":"To ensure that Kubernetes appropriately schedules resources, the respective product values.yaml is configured with default cpu and memory resource request values .","title":"Resource requests and limits"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#resource-requests","text":"The default resource requests that are used for each product are defined below. Note: these values are geared towards small data sets. For larger enterprise deployments refer to the Data Center infrastructure recommendations . Using the formula below, the memory specific values are derived from the default JVM requirements defined for each product's Docker container. Product CPU Memory Jira 2 2G Confluence 2 2G Bitbucket 2 2G Crowd 2 1G","title":"Resource requests"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#synchrony-container-resources","text":"A Confluence cluster requires a Synchrony container. The default resource requests that are used for the Synchrony container are: cpu: 2 memory: 2.5G","title":"Synchrony container resources"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#memory-request-sizing","text":"Request sizing must allow for the size of the product JVM . That means the maximum heap size , minumum heap size and the reserved code cache size (if applicable) plus other JVM overheads, must be considered when defining the request memory size. As a rule of thumb the formula below can be used to deduce the appropriate request memory size. ( maxHeap + codeCache ) * 1 .5","title":"Memory request sizing"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#resource-limits","text":"Environmental and hardware constraints are different for each deployment, therefore the product's values.yaml does not provide a resource limit definition. Resource usage limits can be defined by updating the commented out the resources.container.limits stanza within the appropriate values.yaml . For example: container : limits : cpu : \"2\" memory : \"4G\" requests : cpu : \"2\" memory : \"2G\"","title":"Resource limits"},{"location":"userguide/resource_management/RESOURCE_SCALING/","text":"Product scaling \u00b6 For optimum performance and stability the appropriate resource requests and limits should be defined for each pod. The number of pods in the product cluster should also be carefully considered. Kubernetes provides means for horizontal and vertical scaling of the deployed pods within a cluster, these approaches are described below. Horizontal scaling - adding pods \u00b6 The Helm charts provision one StatefulSet by default. The number of replicas within this StatefulSet can be altered either declaratively or imperatively. Note that the Ingress must support cookie-based session affinity in order for the products to work correctly in a multi-node configuration. Declaratively Update values.yaml by modifying the replicaCount appropriately. Apply the patch: helm upgrade <release> <chart> -f <values file> Imperatively kubectl scale statefulsets <statefulsetset-name> --replicas = n Initial cluster size Jira , Confluence , and Crowd all require manual configuration after the first pod is deployed and before scaling up to additional pods, therefore when you deploy the product only one pod (replica) is created. The initial number of pods that should be started at deployment of each product is set in the replicaCount variable found in the values.yaml and should always be kept as 1. For details on modifying the cpu and memory requirements of the StatefulSet see section Vertical Scaling below. Additional details on the resource requests and limits used by the StatfulSet can be found in Resource requests and limits . Scaling Jira safely \u00b6 At present there are issues relating to index replication with Jira when immediately scaling up by more than 1 pod at a time. See Jira and horizontal scaling . Before scaling your cluster Make sure there's at least one snapshot file in the <shared-home>/export/indexsnapshots directory. New pods will attempt to use the files in this directory to replicate the index. If there is no snapshot present in <shared-home>/export/indexsnapshots then create an initial index snapshot Having followed the steps above, and ensured a healthy snapshot index is available, scale the cluster as necessary . Once scaling is complete confirm that the index is still healthy using the approach prescribed in Step 3 . If there are still indexing issues then please refer to the guides below for details on how address them: Unable to perform a background re-index error Troubleshoot index problems in Jira server Vertical scaling - adding resources \u00b6 The resource requests and limits for a StatefulSet can be defined before product deployment or for deployments that are already running within the Kubernetes cluster. Take note that vertical scaling will result in the pod being re-created with the updated values. Prior to deployment \u00b6 Before performing a helm install update the appropriate products values.yaml container stanza with the desired requests and limits values i.e. container : limits : cpu : \"4\" memory : \"4G\" requests : cpu : \"2\" memory : \"2G\" Post deployment \u00b6 For existing deployments the requests and limits values can be dynamically updated either declaratively or imperatively Declaratively This the preferred approach as it keeps the state of the cluster, and the helm charts themselves in sync. Update values.yaml appropriately Apply the patch: helm upgrade <release> <chart> -f <values file> Imperatively Using kubectl edit on the appropriate StatefulSet the respective cpu and memory values can be modified i.e. resources : requests : cpu : \"2\" memory : 2G Saving the changes will then result in the existing product pod(s) being re-provisioned with the updated values.","title":"Product scaling"},{"location":"userguide/resource_management/RESOURCE_SCALING/#product-scaling","text":"For optimum performance and stability the appropriate resource requests and limits should be defined for each pod. The number of pods in the product cluster should also be carefully considered. Kubernetes provides means for horizontal and vertical scaling of the deployed pods within a cluster, these approaches are described below.","title":"Product scaling"},{"location":"userguide/resource_management/RESOURCE_SCALING/#horizontal-scaling-adding-pods","text":"The Helm charts provision one StatefulSet by default. The number of replicas within this StatefulSet can be altered either declaratively or imperatively. Note that the Ingress must support cookie-based session affinity in order for the products to work correctly in a multi-node configuration. Declaratively Update values.yaml by modifying the replicaCount appropriately. Apply the patch: helm upgrade <release> <chart> -f <values file> Imperatively kubectl scale statefulsets <statefulsetset-name> --replicas = n Initial cluster size Jira , Confluence , and Crowd all require manual configuration after the first pod is deployed and before scaling up to additional pods, therefore when you deploy the product only one pod (replica) is created. The initial number of pods that should be started at deployment of each product is set in the replicaCount variable found in the values.yaml and should always be kept as 1. For details on modifying the cpu and memory requirements of the StatefulSet see section Vertical Scaling below. Additional details on the resource requests and limits used by the StatfulSet can be found in Resource requests and limits .","title":"Horizontal scaling - adding pods"},{"location":"userguide/resource_management/RESOURCE_SCALING/#scaling-jira-safely","text":"At present there are issues relating to index replication with Jira when immediately scaling up by more than 1 pod at a time. See Jira and horizontal scaling . Before scaling your cluster Make sure there's at least one snapshot file in the <shared-home>/export/indexsnapshots directory. New pods will attempt to use the files in this directory to replicate the index. If there is no snapshot present in <shared-home>/export/indexsnapshots then create an initial index snapshot Having followed the steps above, and ensured a healthy snapshot index is available, scale the cluster as necessary . Once scaling is complete confirm that the index is still healthy using the approach prescribed in Step 3 . If there are still indexing issues then please refer to the guides below for details on how address them: Unable to perform a background re-index error Troubleshoot index problems in Jira server","title":"Scaling Jira safely"},{"location":"userguide/resource_management/RESOURCE_SCALING/#vertical-scaling-adding-resources","text":"The resource requests and limits for a StatefulSet can be defined before product deployment or for deployments that are already running within the Kubernetes cluster. Take note that vertical scaling will result in the pod being re-created with the updated values.","title":"Vertical scaling - adding resources"},{"location":"userguide/resource_management/RESOURCE_SCALING/#prior-to-deployment","text":"Before performing a helm install update the appropriate products values.yaml container stanza with the desired requests and limits values i.e. container : limits : cpu : \"4\" memory : \"4G\" requests : cpu : \"2\" memory : \"2G\"","title":"Prior to deployment"},{"location":"userguide/resource_management/RESOURCE_SCALING/#post-deployment","text":"For existing deployments the requests and limits values can be dynamically updated either declaratively or imperatively Declaratively This the preferred approach as it keeps the state of the cluster, and the helm charts themselves in sync. Update values.yaml appropriately Apply the patch: helm upgrade <release> <chart> -f <values file> Imperatively Using kubectl edit on the appropriate StatefulSet the respective cpu and memory values can be modified i.e. resources : requests : cpu : \"2\" memory : 2G Saving the changes will then result in the existing product pod(s) being re-provisioned with the updated values.","title":"Post deployment"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/","text":"Helm chart upgrade \u00b6 Each Helm chart has a default product version that might change in next Helm chart version. So be aware that if you upgrade the Helm chart, it might lead to upgrading the product as well. This depends on the current and target Helm chart versions. Do you want to upgrade the product to a new version? If you want to upgrade the product version without upgrading the Helm chart then refer to Product upgrades . Before upgrading the Helm chart, first consider: the version of the current Helm chart the version of the product running on your Kubernetes cluster the target version of the Helm chart you want to upgrade to the target version of the product you want to upgrade to You need to know if the target product version is zero-downtime compatible (if it is subject to change). Based on this information you may need to choose a different upgrade method. Upgrade product strategies There are two options for upgrade: Normal upgrade : The service will have interruptions during the upgrade. Rolling upgrade : The upgrade will proceed with zero downtime. 1. Find the current version of the installed Helm chart \u00b6 To find the current version of Helm chart and the product version run the following command: helm list --namespace <namespace> You can see the current Helm chart tag version in the CHART column, and the current product tag version in the APP VERSION column for each release name. 2. Define the target Helm chart version \u00b6 Do you have the Atlassian Helm chart repository locally? Make sure you have the Atlassian Helm chart repository in your local Helm repositories. Run the following command to add them: helm repo add atlassian-data-center \\ https://atlassian.github.io/data-center-helm-charts Update the Helm chart repository on your local Helm installation: helm repo update Target Helm chart version The target Helm chart version must be higher than the current Helm chart version. To see all available Helm chart versions of the specific product run this command: helm search repo atlassian-data-center/<product> --versions Select the target Helm chart version. You can find the default application version (target product version tag) in the APP VERSION column. Upgrading the Helm chart to a MAJOR version is not backward compatible. The Helm chart is semantically versioned . You need to take some extra steps if you are upgrading the Helm chart to a MAJOR version. Before you proceed, learn about the steps for your target version in the upgrading section . 3. Define the upgrade method \u00b6 Considering the current and target product versions there are different scenarios: The versions are different, and the target product version is not zero-downtime compatible. The versions are different, and the target product version is zero-downtime compatible. The versions are the same. See the following links to find out if two versions of a product are zero-downtime compatible Jira: Upgrading Jira with zero downtime Confluence: Upgrading Confluence with zero downtime Bitbucket: Upgrading Bitbucket with zero downtime All supported Jira versions are zero-downtime compatible The minimum supported version of Jira in the Data Center Helm Charts is 8.19 . Considering any Jira version later than 8.x is zero-downtime compatible, all supported Jira Data Center versions are zero-downtime compatible. Based on the scenario follow one of these options in the next step: Normal upgrade : Upgrade Helm chart when the target product version is not zero-downtime compatible, or you want to avoid mixed version during the upgrade. In this option the product will have a downtime during the upgrade process. Rolling upgrade : Upgrade Helm chart when the target product version is zero-downtime compatible. This option will only apply when the target product version is zero-downtime compatible. If you are not sure about this see the links above. No product upgrade : Upgrade the Helm chart with no change in product version. We recommend this method when the target product version is the same as the current product version, or for any other reason you may not want to change the product version but still upgrade the helm chart. 4. Upgrade the Helm chart \u00b6 Tip: Monitor the pods during the upgrade process You can monitor the pod activities during the upgrade by running the following command in a separate terminal: kubectl get pods --namespace <namespace> --watch Normal upgrade Helm chart upgrade with downtime \u00b6 You need to use this method to upgrade the Helm chart if: * the target product version is not zero downtime-compatible * for any other reason you would prefer to avoid running the cluster in mix mode Upgrading the Helm chart might change the product version If you want to upgrade the Helm chart to a newer version but don't want to change the product version then follow the Upgrade with no change in product version tab. The strategy for upgrading the product with downtime is to scale down the cluster to zero nodes and then start the nodes with new product versions. And finally scale the cluster up to the original number of nodes. Here are step-by-step instructions for the upgrade process: Find out the number of nodes in the cluster. kubectl describe sts <release-name> --namespace <namespace> | grep 'Replicas' Upgrade the Helm chart. Replace the product name in the following command: helm upgrade <release-name> atlassian-data-center/<product> \\ --version <target-helm-chart-version>> \\ --reuse-values \\ --replicaCount = 1 \\ --wait \\ --namespace <namespace> The cluster will scale down to zero nodes. Then one pod with the target product version will be recreated and join the cluster. Scale up the cluster. After you confirm the new pod is in Running status then scale up the cluster to the same number of nodes as before the upgrade: helm upgrade <release-name> atlassian-data-center/confluence \\ --reuse-values \\ --replicaCount = <n> \\ --wait \\ --namespace <namespace> Rolling upgrade Helm chart upgrade with zero downtime \u00b6 Upgrade the Helm chart might change the product version If you want to upgrade the Helm chart to newer version but don't want to change the product version then follow the Upgrade with no change in product version tab. Rolling upgrade is not possible if the cluster has only one node If you have just one node in the cluster then you can't take advantage of the zero-downtime approach. You may scale up the cluster to at least two nodes before upgrading or there will be a downtime during the upgrade. In order to upgrade the Helm chart when the target product version is different from the current product version, you can use upgrade with zero downtime to avoid any service interruption during the upgrade. To use this option the target version must be zero-downtime compatible. Make sure the product target version is zero downtime-compatible To ensure you will have a smooth upgrade make sure the product target version is zero-downtime compatible. If you still aren't sure about this go back to step 3. Here are the step-by-step instructions of the upgrade process. These steps may vary for each product: Jira Put Jira into upgrade mode. Go to Administration > Applications > Jira upgrades and click Put Jira into upgrade mode . Run the upgrade using Helm. helm upgrade <release-name> atlassian-data-center/jira \\ --version <target-helm-chart-version>> \\ --reuse-values \\ --wait \\ --namespace <namespace> Wait for the upgrade to finish. The pods will be recreated with the updated version, one at a time. Finalize the upgrade. After all the pods are active with the new version, click Run upgrade tasks to finalize the upgrade: Confluence Put Confluence into upgrade mode. From the admin page click on Rolling Upgrade and set the Confluence in Upgrade mode: Run the upgrade using Helm. helm upgrade <release-name> atlassian-data-center/confluence \\ --version <target-helm-chart-version>> \\ --reuse-values \\ --wait \\ --namespace <namespace> Wait until all pods are recreated and are back to Running status. Wait for the upgrade to finish. The pods will be recreated with the updated version, one at a time. Finalize the upgrade. After all the pods are active with the new version, click Run upgrade tasks to finalize the upgrade: Bitbucket Put Bitbucket into upgrade mode. From the admin page click on Rolling Upgrade and set the Bitbucket in Upgrade mode: Run the upgrade using Helm. helm upgrade <release-name> atlassian-data-center/bitbucket \\ --version <target-helm-chart-version>> \\ --reuse-values \\ --wait \\ --namespace <namespace> Wait until all pods are recreated and are back to Running status. Wait for the upgrade to finish. The pods will be recreated with the updated version, one at a time. Finalize the upgrade. After all the pods are active with the new version, click Run upgrade tasks to finalize the upgrade: Upgrade with no change in the product version Helm chart upgrade with no change in product version \u00b6 If your target Helm chart has a different product version in comparison with the current product version, and you still want to keep the current product version unchanged, you should use the following command to upgrade the Helm chart: helm upgrade <release-name> atlassian-data-center/<product> \\ --version <helm-chart-target-version> \\ --reuse-values \\ --set image.tag = <current-product-tag> \\ --wait \\ --namespace <namespace> However, when the product versions of target and current Helm charts are the same, then you can run the following command to upgrade the Helm chart only: helm upgrade <release-name> atlassian-data-center/<product> \\ --version <helm-chart-target-version> \\ --reuse-values \\ --wait \\ --namespace <namespace>","title":"Helm chart upgrade"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#helm-chart-upgrade","text":"Each Helm chart has a default product version that might change in next Helm chart version. So be aware that if you upgrade the Helm chart, it might lead to upgrading the product as well. This depends on the current and target Helm chart versions. Do you want to upgrade the product to a new version? If you want to upgrade the product version without upgrading the Helm chart then refer to Product upgrades . Before upgrading the Helm chart, first consider: the version of the current Helm chart the version of the product running on your Kubernetes cluster the target version of the Helm chart you want to upgrade to the target version of the product you want to upgrade to You need to know if the target product version is zero-downtime compatible (if it is subject to change). Based on this information you may need to choose a different upgrade method. Upgrade product strategies There are two options for upgrade: Normal upgrade : The service will have interruptions during the upgrade. Rolling upgrade : The upgrade will proceed with zero downtime.","title":"Helm chart upgrade"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#1-find-the-current-version-of-the-installed-helm-chart","text":"To find the current version of Helm chart and the product version run the following command: helm list --namespace <namespace> You can see the current Helm chart tag version in the CHART column, and the current product tag version in the APP VERSION column for each release name.","title":"1. Find the current version of the installed Helm chart"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#2-define-the-target-helm-chart-version","text":"Do you have the Atlassian Helm chart repository locally? Make sure you have the Atlassian Helm chart repository in your local Helm repositories. Run the following command to add them: helm repo add atlassian-data-center \\ https://atlassian.github.io/data-center-helm-charts Update the Helm chart repository on your local Helm installation: helm repo update Target Helm chart version The target Helm chart version must be higher than the current Helm chart version. To see all available Helm chart versions of the specific product run this command: helm search repo atlassian-data-center/<product> --versions Select the target Helm chart version. You can find the default application version (target product version tag) in the APP VERSION column. Upgrading the Helm chart to a MAJOR version is not backward compatible. The Helm chart is semantically versioned . You need to take some extra steps if you are upgrading the Helm chart to a MAJOR version. Before you proceed, learn about the steps for your target version in the upgrading section .","title":"2. Define the target Helm chart version"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#3-define-the-upgrade-method","text":"Considering the current and target product versions there are different scenarios: The versions are different, and the target product version is not zero-downtime compatible. The versions are different, and the target product version is zero-downtime compatible. The versions are the same. See the following links to find out if two versions of a product are zero-downtime compatible Jira: Upgrading Jira with zero downtime Confluence: Upgrading Confluence with zero downtime Bitbucket: Upgrading Bitbucket with zero downtime All supported Jira versions are zero-downtime compatible The minimum supported version of Jira in the Data Center Helm Charts is 8.19 . Considering any Jira version later than 8.x is zero-downtime compatible, all supported Jira Data Center versions are zero-downtime compatible. Based on the scenario follow one of these options in the next step: Normal upgrade : Upgrade Helm chart when the target product version is not zero-downtime compatible, or you want to avoid mixed version during the upgrade. In this option the product will have a downtime during the upgrade process. Rolling upgrade : Upgrade Helm chart when the target product version is zero-downtime compatible. This option will only apply when the target product version is zero-downtime compatible. If you are not sure about this see the links above. No product upgrade : Upgrade the Helm chart with no change in product version. We recommend this method when the target product version is the same as the current product version, or for any other reason you may not want to change the product version but still upgrade the helm chart.","title":"3. Define the upgrade method"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#4-upgrade-the-helm-chart","text":"Tip: Monitor the pods during the upgrade process You can monitor the pod activities during the upgrade by running the following command in a separate terminal: kubectl get pods --namespace <namespace> --watch Normal upgrade","title":"4. Upgrade the Helm chart"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#helm-chart-upgrade-with-downtime","text":"You need to use this method to upgrade the Helm chart if: * the target product version is not zero downtime-compatible * for any other reason you would prefer to avoid running the cluster in mix mode Upgrading the Helm chart might change the product version If you want to upgrade the Helm chart to a newer version but don't want to change the product version then follow the Upgrade with no change in product version tab. The strategy for upgrading the product with downtime is to scale down the cluster to zero nodes and then start the nodes with new product versions. And finally scale the cluster up to the original number of nodes. Here are step-by-step instructions for the upgrade process: Find out the number of nodes in the cluster. kubectl describe sts <release-name> --namespace <namespace> | grep 'Replicas' Upgrade the Helm chart. Replace the product name in the following command: helm upgrade <release-name> atlassian-data-center/<product> \\ --version <target-helm-chart-version>> \\ --reuse-values \\ --replicaCount = 1 \\ --wait \\ --namespace <namespace> The cluster will scale down to zero nodes. Then one pod with the target product version will be recreated and join the cluster. Scale up the cluster. After you confirm the new pod is in Running status then scale up the cluster to the same number of nodes as before the upgrade: helm upgrade <release-name> atlassian-data-center/confluence \\ --reuse-values \\ --replicaCount = <n> \\ --wait \\ --namespace <namespace> Rolling upgrade","title":"Helm chart upgrade with downtime"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#helm-chart-upgrade-with-zero-downtime","text":"Upgrade the Helm chart might change the product version If you want to upgrade the Helm chart to newer version but don't want to change the product version then follow the Upgrade with no change in product version tab. Rolling upgrade is not possible if the cluster has only one node If you have just one node in the cluster then you can't take advantage of the zero-downtime approach. You may scale up the cluster to at least two nodes before upgrading or there will be a downtime during the upgrade. In order to upgrade the Helm chart when the target product version is different from the current product version, you can use upgrade with zero downtime to avoid any service interruption during the upgrade. To use this option the target version must be zero-downtime compatible. Make sure the product target version is zero downtime-compatible To ensure you will have a smooth upgrade make sure the product target version is zero-downtime compatible. If you still aren't sure about this go back to step 3. Here are the step-by-step instructions of the upgrade process. These steps may vary for each product: Jira Put Jira into upgrade mode. Go to Administration > Applications > Jira upgrades and click Put Jira into upgrade mode . Run the upgrade using Helm. helm upgrade <release-name> atlassian-data-center/jira \\ --version <target-helm-chart-version>> \\ --reuse-values \\ --wait \\ --namespace <namespace> Wait for the upgrade to finish. The pods will be recreated with the updated version, one at a time. Finalize the upgrade. After all the pods are active with the new version, click Run upgrade tasks to finalize the upgrade: Confluence Put Confluence into upgrade mode. From the admin page click on Rolling Upgrade and set the Confluence in Upgrade mode: Run the upgrade using Helm. helm upgrade <release-name> atlassian-data-center/confluence \\ --version <target-helm-chart-version>> \\ --reuse-values \\ --wait \\ --namespace <namespace> Wait until all pods are recreated and are back to Running status. Wait for the upgrade to finish. The pods will be recreated with the updated version, one at a time. Finalize the upgrade. After all the pods are active with the new version, click Run upgrade tasks to finalize the upgrade: Bitbucket Put Bitbucket into upgrade mode. From the admin page click on Rolling Upgrade and set the Bitbucket in Upgrade mode: Run the upgrade using Helm. helm upgrade <release-name> atlassian-data-center/bitbucket \\ --version <target-helm-chart-version>> \\ --reuse-values \\ --wait \\ --namespace <namespace> Wait until all pods are recreated and are back to Running status. Wait for the upgrade to finish. The pods will be recreated with the updated version, one at a time. Finalize the upgrade. After all the pods are active with the new version, click Run upgrade tasks to finalize the upgrade: Upgrade with no change in the product version","title":"Helm chart upgrade with zero downtime"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#helm-chart-upgrade-with-no-change-in-product-version","text":"If your target Helm chart has a different product version in comparison with the current product version, and you still want to keep the current product version unchanged, you should use the following command to upgrade the Helm chart: helm upgrade <release-name> atlassian-data-center/<product> \\ --version <helm-chart-target-version> \\ --reuse-values \\ --set image.tag = <current-product-tag> \\ --wait \\ --namespace <namespace> However, when the product versions of target and current Helm charts are the same, then you can run the following command to upgrade the Helm chart only: helm upgrade <release-name> atlassian-data-center/<product> \\ --version <helm-chart-target-version> \\ --reuse-values \\ --wait \\ --namespace <namespace>","title":"Helm chart upgrade with no change in product version"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/","text":"Products upgrade \u00b6 We recommend upgrading the Helm chart rather than upgrading the product directly. However, if you want to upgrade the product to a specific version that is not listed in the Helm charts, or if you don't want to upgrade Helm chart to a newer version but you still need to upgrade the product version, then you are in a right place. To upgrade the product to a newer version without upgrading the Helm chart follow these steps: 1. Find the tag of the target image \u00b6 Go to the Atlassian Docker Hub page of the relevant product to pick a tag that matches your target version. Atlassian Docker Hub page for supported products: Jira: atlassian/jira-software Confluence: atlassian/confluence-server Bitbucket: atlassian/bitbucket-server In the example you're running Jira using the 8.13.0-jdk11 tag, and you'll be upgrading to 8.13.1-jdk11 - our target . 2. Define the upgrade strategy \u00b6 There are two strategies to upgrade the application: Normal upgrade : The service will have interruptions during the upgrade. Rolling upgrade : The upgrade will proceed with zero downtime. You can use rolling upgrade only if the target version is zero-downtime compatible. Can you use the rolling upgrade option? To confirm if you can run a rolling upgrade option, check your current and target product versions in the relevant link: Jira: Upgrading Jira with zero downtime Confluence: Upgrading Confluence with zero downtime Bitbucket: Upgrading Bitbucket with zero downtime 3. Upgrade the product \u00b6 Normal Upgrade Normal upgrade \u00b6 The service will have interruptions during the normal upgrade You can use this method to upgrade the Helm chart if: The target product version is not zero-downtime compatible If you prefer to avoid running the cluster in mix mode The strategy for normal upgrading is to scale down the cluster to zero nodes, and then start one node with the new product version. Then scale up the cluster to the original number of nodes. Here are the step-by-step instructions for the upgrade process: Find out the current number of nodes. Run the following command: kubectl describe sts <release-name> --namespace <namespace> | grep 'Replicas' Run the upgrade using Helm. Based on the product you want to upgrade replace the product name in the following command and run: helm upgrade <release-name> atlassian-data-center/<product> \\ --reuse-values \\ --replicaCount = 1 \\ --set image.tag = <target-tag> \\ --wait \\ --namespace <namespace> The cluster will scale down to zero nodes. Then one pod with the target product version will be recreated and join the cluster. Scale up the cluster. After you confirm the new pod is in Running status then scale up the cluster to the same number of nodes as before the upgrade: helm upgrade <release-name> atlassian-data-center/confluence \\ --reuse-values \\ --replicaCount = <n> \\ --wait \\ --namespace <namespace> Rolling upgrade Rolling (zero downtime) upgrade \u00b6 Select the product tab to upgrade Upgrading the product with zero downtime is bit different for each product. Please select the product and follow the steps to complete the rolling upgrade. Jira Jira rolling upgrade \u00b6 Let's say we have Jira version 8.19.0 deployed to our Kubernetes cluster, and we want to upgrade it to version 8.19.1 , which we'll call the target version . You can substitute the target version for the one you need, as long as it's newer than the current one. 1. Find tag of the target image. \u00b6 Go to atlassian/jira-software Docker Hub page to pick a tag that matches your target version. In the example we're running Jira using the 8.19.0-jdk11 tag, and we'll be upgrading to 8.19.1-jdk11 - our target . 2. Put Jira into upgrade mode. \u00b6 Go to Administration > Applications > Jira upgrades and click Put Jira into upgrade mode . 3. Run the upgrade using Helm. \u00b6 Run the Helm upgrade command with your release name ( <release-name> ) and the target image from a previous step ( <target-tag> ). For more details, refer to the Helm documentation . helm upgrade <release-name> atlassian-data-center/jira \\ --wait \\ --reuse-values \\ --set image.tag = <target-tag> If you used kubectl scale after installing the Helm chart, you'll need to add --set replicaCount=<number-of-jira-nodes> to the command. Otherwise, the deployment will be scaled back to the original number, which most likely is one node. 4. Wait for the upgrade to finish. \u00b6 The pods will be recreated with the updated version, one at a time. 5. Finalize the upgrade. \u00b6 After all pods are active with the new version, click Run upgrade tasks to finalize the upgrade: Confluence Confluence rolling upgrade \u00b6 Let's say we have Confluence version 7.12.0 deployed to our Kubernetes cluster, and we want to upgrade it to version 7.12.1 , which we'll call the target version . You can substitute the target version for the one you need, as long as it's newer than the current one. Follow the link to confirm the target version is zero-downtime compatible Upgrading Confluence with zero downtime 1. Find the tag of the target image. \u00b6 Go to atlassian/confluence-server Docker Hub page to pick a tag that matches your target version. In the example we're running Confluence using the 7.12.0-jdk11 tag, and we'll be upgrading to 7.12.1-jdk11 - our target . 2. Put Confluence into upgrade mode. \u00b6 From the admin page click on Rolling Upgrade and set the Confluence in Upgrade mode: 3. Run the upgrade using Helm. \u00b6 Run the Helm upgrade command with your release name ( <release-name> ) and the target image from a previous step ( <target-tag> ). For more details, refer to the Helm documentation . helm upgrade <release-name> atlassian-data-center/confluence \\ --wait \\ --reuse-values \\ --set image.tag = <target-tag> If you used kubectl scale after installing the Helm chart, you'll need to add --set replicaCount=<number-of-confluence-nodes> to the command. Otherwise, the deployment will be scaled back to the original number, which most likely is one node. 4. Wait for the upgrade to finish. \u00b6 The pods will be recreated with the updated version, one at a time. 5. Finalize the upgrade. \u00b6 After all the pods are activated with the new version, finalize the upgrade: Bitbucket Bitbucket rolling upgrade \u00b6 Let's say we have Bitbucket version 7.12.0 deployed to our Kubernetes cluster, and we want to upgrade it to version 7.12.1 , which we'll call the target version . You can substitute the target version for the one you need, as long as it's newer than the current one. Follow the link to find out if the target version is zero-downtime compatible Upgrading Bitbucket with zero downtime 1. Find tag of the target image. \u00b6 Go to atlassian/bitbucket-server Docker Hub page to pick a tag that matches your target version. In the example we're running Bitbucket using the 7.12.0-jdk11 tag, and we'll be upgrading to 7.12.1-jdk11 - our target . 2. Put Bitbucket into upgrade mode. \u00b6 From the admin page click on Rolling Upgrade and set the Bitbucket to Upgrade mode: 3. Run the upgrade using Helm. \u00b6 Run the Helm upgrade command with your release name ( <release-name> ) and the target image from a previous step ( <target-tag> ). For more details, consult the Helm documentation . helm upgrade <release-name> atlassian-data-center/bitbucket \\ --wait \\ --reuse-values \\ --set image.tag = <target-tag> If you used kubectl scale after installing the Helm chart, you'll need to add --set replicaCount=<number-of-bb-nodes> to the command. Otherwise, the deployment will be scaled back to the original number, which most likely is one node. 4. Wait for the upgrade to finish. \u00b6 The pods will be recreated with the updated version, one at a time. 5. Finalize the upgrade. \u00b6 After all the pods are active with the new version, finalize the upgrade:","title":"Products upgrade"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#products-upgrade","text":"We recommend upgrading the Helm chart rather than upgrading the product directly. However, if you want to upgrade the product to a specific version that is not listed in the Helm charts, or if you don't want to upgrade Helm chart to a newer version but you still need to upgrade the product version, then you are in a right place. To upgrade the product to a newer version without upgrading the Helm chart follow these steps:","title":"Products upgrade"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#1-find-the-tag-of-the-target-image","text":"Go to the Atlassian Docker Hub page of the relevant product to pick a tag that matches your target version. Atlassian Docker Hub page for supported products: Jira: atlassian/jira-software Confluence: atlassian/confluence-server Bitbucket: atlassian/bitbucket-server In the example you're running Jira using the 8.13.0-jdk11 tag, and you'll be upgrading to 8.13.1-jdk11 - our target .","title":"1. Find the tag of the target image"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#2-define-the-upgrade-strategy","text":"There are two strategies to upgrade the application: Normal upgrade : The service will have interruptions during the upgrade. Rolling upgrade : The upgrade will proceed with zero downtime. You can use rolling upgrade only if the target version is zero-downtime compatible. Can you use the rolling upgrade option? To confirm if you can run a rolling upgrade option, check your current and target product versions in the relevant link: Jira: Upgrading Jira with zero downtime Confluence: Upgrading Confluence with zero downtime Bitbucket: Upgrading Bitbucket with zero downtime","title":"2. Define the upgrade strategy"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#3-upgrade-the-product","text":"Normal Upgrade","title":"3. Upgrade the product"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#normal-upgrade","text":"The service will have interruptions during the normal upgrade You can use this method to upgrade the Helm chart if: The target product version is not zero-downtime compatible If you prefer to avoid running the cluster in mix mode The strategy for normal upgrading is to scale down the cluster to zero nodes, and then start one node with the new product version. Then scale up the cluster to the original number of nodes. Here are the step-by-step instructions for the upgrade process: Find out the current number of nodes. Run the following command: kubectl describe sts <release-name> --namespace <namespace> | grep 'Replicas' Run the upgrade using Helm. Based on the product you want to upgrade replace the product name in the following command and run: helm upgrade <release-name> atlassian-data-center/<product> \\ --reuse-values \\ --replicaCount = 1 \\ --set image.tag = <target-tag> \\ --wait \\ --namespace <namespace> The cluster will scale down to zero nodes. Then one pod with the target product version will be recreated and join the cluster. Scale up the cluster. After you confirm the new pod is in Running status then scale up the cluster to the same number of nodes as before the upgrade: helm upgrade <release-name> atlassian-data-center/confluence \\ --reuse-values \\ --replicaCount = <n> \\ --wait \\ --namespace <namespace> Rolling upgrade","title":"Normal upgrade"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#rolling-zero-downtime-upgrade","text":"Select the product tab to upgrade Upgrading the product with zero downtime is bit different for each product. Please select the product and follow the steps to complete the rolling upgrade. Jira","title":"Rolling (zero downtime) upgrade"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#jira-rolling-upgrade","text":"Let's say we have Jira version 8.19.0 deployed to our Kubernetes cluster, and we want to upgrade it to version 8.19.1 , which we'll call the target version . You can substitute the target version for the one you need, as long as it's newer than the current one.","title":"Jira rolling upgrade"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#1-find-tag-of-the-target-image","text":"Go to atlassian/jira-software Docker Hub page to pick a tag that matches your target version. In the example we're running Jira using the 8.19.0-jdk11 tag, and we'll be upgrading to 8.19.1-jdk11 - our target .","title":"1. Find tag of the target image."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#2-put-jira-into-upgrade-mode","text":"Go to Administration > Applications > Jira upgrades and click Put Jira into upgrade mode .","title":"2. Put Jira into upgrade mode."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#3-run-the-upgrade-using-helm","text":"Run the Helm upgrade command with your release name ( <release-name> ) and the target image from a previous step ( <target-tag> ). For more details, refer to the Helm documentation . helm upgrade <release-name> atlassian-data-center/jira \\ --wait \\ --reuse-values \\ --set image.tag = <target-tag> If you used kubectl scale after installing the Helm chart, you'll need to add --set replicaCount=<number-of-jira-nodes> to the command. Otherwise, the deployment will be scaled back to the original number, which most likely is one node.","title":"3. Run the upgrade using Helm."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#4-wait-for-the-upgrade-to-finish","text":"The pods will be recreated with the updated version, one at a time.","title":"4. Wait for the upgrade to finish."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#5-finalize-the-upgrade","text":"After all pods are active with the new version, click Run upgrade tasks to finalize the upgrade: Confluence","title":"5. Finalize the upgrade."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#confluence-rolling-upgrade","text":"Let's say we have Confluence version 7.12.0 deployed to our Kubernetes cluster, and we want to upgrade it to version 7.12.1 , which we'll call the target version . You can substitute the target version for the one you need, as long as it's newer than the current one. Follow the link to confirm the target version is zero-downtime compatible Upgrading Confluence with zero downtime","title":"Confluence rolling upgrade"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#1-find-the-tag-of-the-target-image_1","text":"Go to atlassian/confluence-server Docker Hub page to pick a tag that matches your target version. In the example we're running Confluence using the 7.12.0-jdk11 tag, and we'll be upgrading to 7.12.1-jdk11 - our target .","title":"1. Find the tag of the target image."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#2-put-confluence-into-upgrade-mode","text":"From the admin page click on Rolling Upgrade and set the Confluence in Upgrade mode:","title":"2. Put Confluence into upgrade mode."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#3-run-the-upgrade-using-helm_1","text":"Run the Helm upgrade command with your release name ( <release-name> ) and the target image from a previous step ( <target-tag> ). For more details, refer to the Helm documentation . helm upgrade <release-name> atlassian-data-center/confluence \\ --wait \\ --reuse-values \\ --set image.tag = <target-tag> If you used kubectl scale after installing the Helm chart, you'll need to add --set replicaCount=<number-of-confluence-nodes> to the command. Otherwise, the deployment will be scaled back to the original number, which most likely is one node.","title":"3. Run the upgrade using Helm."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#4-wait-for-the-upgrade-to-finish_1","text":"The pods will be recreated with the updated version, one at a time.","title":"4. Wait for the upgrade to finish."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#5-finalize-the-upgrade_1","text":"After all the pods are activated with the new version, finalize the upgrade: Bitbucket","title":"5. Finalize the upgrade."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#bitbucket-rolling-upgrade","text":"Let's say we have Bitbucket version 7.12.0 deployed to our Kubernetes cluster, and we want to upgrade it to version 7.12.1 , which we'll call the target version . You can substitute the target version for the one you need, as long as it's newer than the current one. Follow the link to find out if the target version is zero-downtime compatible Upgrading Bitbucket with zero downtime","title":"Bitbucket rolling upgrade"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#1-find-tag-of-the-target-image_1","text":"Go to atlassian/bitbucket-server Docker Hub page to pick a tag that matches your target version. In the example we're running Bitbucket using the 7.12.0-jdk11 tag, and we'll be upgrading to 7.12.1-jdk11 - our target .","title":"1. Find tag of the target image."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#2-put-bitbucket-into-upgrade-mode","text":"From the admin page click on Rolling Upgrade and set the Bitbucket to Upgrade mode:","title":"2. Put Bitbucket into upgrade mode."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#3-run-the-upgrade-using-helm_2","text":"Run the Helm upgrade command with your release name ( <release-name> ) and the target image from a previous step ( <target-tag> ). For more details, consult the Helm documentation . helm upgrade <release-name> atlassian-data-center/bitbucket \\ --wait \\ --reuse-values \\ --set image.tag = <target-tag> If you used kubectl scale after installing the Helm chart, you'll need to add --set replicaCount=<number-of-bb-nodes> to the command. Otherwise, the deployment will be scaled back to the original number, which most likely is one node.","title":"3. Run the upgrade using Helm."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#4-wait-for-the-upgrade-to-finish_2","text":"The pods will be recreated with the updated version, one at a time.","title":"4. Wait for the upgrade to finish."},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#5-finalize-the-upgrade_2","text":"After all the pods are active with the new version, finalize the upgrade:","title":"5. Finalize the upgrade."}]}